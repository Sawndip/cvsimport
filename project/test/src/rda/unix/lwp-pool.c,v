head	1.8;
access;
symbols
	msnyder-fork-checkpoint-branch:1.6.0.2
	msnyder-fork-checkpoint-branchpoint:1.6
	jimb-rda-nptl-branch:1.1.0.2;
locks; strict;
comment	@ * @;


1.8
date	2012.06.14.20.21.57;	author kevinb;	state Exp;
branches;
next	1.7;

1.7
date	2010.10.14.18.08.13;	author kevinb;	state Exp;
branches;
next	1.6;

1.6
date	2005.11.11.20.29.57;	author kevinb;	state Exp;
branches;
next	1.5;

1.5
date	2005.11.10.21.32.15;	author kevinb;	state Exp;
branches;
next	1.4;

1.4
date	2005.11.09.02.16.46;	author kevinb;	state Exp;
branches;
next	1.3;

1.3
date	2005.11.08.21.58.36;	author kevinb;	state Exp;
branches;
next	1.2;

1.2
date	2005.06.30.03.24.18;	author jimb;	state Exp;
branches;
next	1.1;

1.1
date	2004.11.23.06.02.19;	author jimb;	state dead;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2004.11.23.06.02.19;	author jimb;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2004.11.29.19.13.39;	author jimb;	state Exp;
branches;
next	1.1.2.3;

1.1.2.3
date	2004.11.29.19.36.22;	author jimb;	state Exp;
branches;
next	1.1.2.4;

1.1.2.4
date	2004.12.02.23.46.46;	author jimb;	state Exp;
branches;
next	1.1.2.5;

1.1.2.5
date	2004.12.02.23.47.46;	author jimb;	state Exp;
branches;
next	1.1.2.6;

1.1.2.6
date	2004.12.03.00.14.51;	author jimb;	state Exp;
branches;
next	1.1.2.7;

1.1.2.7
date	2004.12.03.21.36.40;	author jimb;	state Exp;
branches;
next	1.1.2.8;

1.1.2.8
date	2004.12.03.23.35.45;	author jimb;	state Exp;
branches;
next	;


desc
@@


1.8
log
@	* lwp-pool.c (struct lwp): Add new field `disabled'.
	(empty_lwp_slot): New static global.
	(hash_empty_slot, resize_hash, lwp_pool_stop_all)
	(lwp_pool_continue_all, clear_all_do_step_flags)
	(hash_find): Rename to hash_find_1.  Add parameter `create_p'.
	Initialize `disabled' field.
	(hash_find, hash_find_no_create, lwp_pool_disable_lwp)
	(lwp_pool_enable_lwp): New functions.
	(hash_delete): Revise method used for deleting a slot from
	the hash table.
	(lwp_pool_continue_all): Don't continue disabled LWPs.
	(lwp_pool_continue_lwp): Print a warning instead of an error, and
	then only when LWP Pool diagnostics are enabled when attempting
	to continue a LWP with the state of
	lwp_state_stopped_stop_pending_interesting.
	* lwp-pool.h (lwp_pool_disable_lwp, lwp_pool_enable_lwp): Declare.
	* thread-db.c (find_new_threads_callback): Adjust thread
	deletion / reuse message.
	(update_thread_list): Disable continuation of zombie threads.
	(thread_db_break_program): Enable diagnostic message for "monitor
	thread-db-noisy".  Use kill_lwp() instead of kill().
	(thread_db_check_child_state): Don't stop on signals from zombie
	threads.
@
text
@/* lwp-pool.c --- implementation of a stoppable, waitable LWP pool.

   Copyright 2004 Red Hat, Inc.

   This file is part of RDA, the Red Hat Debug Agent (and library).

   This program is free software; you can redistribute it and/or modify
   it under the terms of the GNU General Public License as published by
   the Free Software Foundation; either version 2 of the License, or
   (at your option) any later version.

   This program is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You should have received a copy of the GNU General Public License
   along with this program; if not, write to the Free Software
   Foundation, Inc., 59 Temple Place - Suite 330,
   Boston, MA 02111-1307, USA.
   
   Alternative licenses for RDA may be arranged by contacting Red Hat,
   Inc.  */

#include "config.h"

#define _GNU_SOURCE /* for strerror */

#include <assert.h>
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include <errno.h>
#include <sys/types.h>
#include <sys/wait.h>

#include "lwp-pool.h"
#include "lwp-ctrl.h"

#include "diagnostics.h"

int debug_lwp_pool = 0;


/* THE LIFETIME OF A TRACED LWP

   POSIX uses these terms in talking about signals:

   - To "generate" a signal is to call kill or raise, divide by zero,
     etc.

   - To "deliver" a signal is to do whatever that signal's designated
     action is: ignore it, enter a signal handler, terminate the
     process, or stop the process.

   - To "accept" a signal is to have 'sigwait' or a similar function
     select and return the signal.

   - A signal is "pending" between the time it is generated and the
     time it is delivered.

   So, here is the life cycle of a traced LWP:

   - It is created by fork or vfork and does a PTRACE_TRACEME.  The
     PTRACE_TRACEME makes it a traced, running LWP.  When a traced LWP
     does an exec, it gets a SIGTRAP before executing the first
     instruction in the new process image, so the LWP will then stop.

     Or, we attach to it with a PTRACE_ATTACH.  This sends a SIGSTOP
     to the LWP, so it will stop.

   - While a traced LWP is stopped, we can read and write its
     registers and memory.  We can also send it signals; they become
     pending on the LWP, and will be reported by waitpid.

   - A stopped LWP can be set running again in one of two ways:

     + by doing a PTRACE_CONT, PTRACE_SINGLESTEP, or PTRACE_SYSCALL; or

     + by sending it a SIGCONT.

     The ptrace requests all let you specify a signal to be delivered
     to the process.  This is the only way signals (other than
     SIGKILL) ever get actually delivered: every other signal just
     gets reported to the debugger via waitpid when delivery is
     attempted.

     Sending a SIGCONT clears any pending SIGSTOPs; PTRACE_CONT and
     PTRACE_SINGLESTEP don't have that side effect.

     (Sending an LWP a SIGKILL via the 'kill' or 'tkill' system calls
     acts like sending it a SIGKILL followed by a SIGCONT.)

   - A running LWP may exit or be terminated by a signal at any time,
     so accessing its memory or registers or sending it a signal is
     always a race.

   - waitpid will eventually return a status S for a continued LWP:

     + If WIFEXITED (S) or WIFSIGNALED (S), the LWP no longer exists.
     
     + IF WIFSTOPPED (S), the LWP is stopped again, because some
       signal WSTOPSIG (S) was about to be delivered to it.  Here we
       go back to the second step.

       Note that the signal WSTOPSIG (S) has not yet been delivered to
       the process, and is no longer pending on the process.  Only
       signals passed to the ptrace requests get delivered.  In
       effect, the debugger gets to intercept signals before they are
       delivered, and decide whether to pass them through or not.
       (The exception is SIGKILL: that always produces a WIFSIGNALED
       wait status, and terminates the process.)

   So, to put all that together:

   - A traced LWP goes back and forth from running to stopped, until
     eventually it goes from running to exited or killed.

   - Running->stopped transitions are always attempted signal
     deliveries, yielding WIFSTOPPED wait statuses.

   - Stopping->running transitions are generally due to ptrace
     requests by the debugger.  (The debugger could use kill to send
     SIGCONT, but that's messy.)

   - Running->exited transitions are due to, duh, the LWP exiting.

   - Running->killed transitions are due to a signal being delivered
     to the LWP that is neither ignored nor caught.


   Under NPTL, this life cycle is a bit different: LWPs simply exit,
   without creating a zombie; they produce no wait status.  The NPTL
   libthread_db generates a TD_DEATH event for them, but at the kernel
   level the only indication that they're gone is that the tkill
   system call fails with ESRCH ("No such process").

   Under LinuxThreads, LWPs remain zombie processes until they're
   waited for.  Attempts to send them signals while zombies have no
   effect, but return no error.


   STOPPING A PROCESS

   The major challenge here is implementing the lwp_pool_stop_all
   function.  The only way to stop a running LWP is to send it a
   SIGSTOP, and then wait for a status acknowledging the stop.  But as
   explained above, a running LWP could stop at any time of its own
   accord, so sending it a SIGSTOP is always a race.  By the time you
   call waitpid, you don't know whether you'll get a status for the
   SIGSTOP you just sent, or for something else: some other signal, an
   exit, or a termination by signal.

   If the LWP turns out to have exited or died, then that's pretty
   easy to handle.  Your attempt to send a SIGSTOP will get an error,
   and then you'll get a wait status for the termination.  A
   termination status is always the last status you'll get from wait
   for that LWP, so there'll be no further record of your SIGSTOP.

   If the LWP was about to have some other signal delivered to it,
   then the next wait will return a WIFSTOPPED status for that signal;
   we'll have to continue the LWP and wait again until we get the
   status for our SIGSTOP.  The kernel forgets about any signals the
   LWP has received once it has reported them to us, so it's up to us
   to keep track of them and report them via lwp_pool_waitpid.  */



/* The LWP structure.  */

/* The states an LWP we're managing might be in.

   For the purposes of these states, we classify wait statuses as
   follows:

   - An "interesting" wait status is one that isn't a result of us
     attaching to the LWP or sending it a SIGSTOP for
     lwp_pool_stop_all.  It indicates something that happened to the
     LWP other than as a result of this code's fiddling with it.  We
     report all interesting wait statuses via lwp_pool_waitpid.

   - A "boring" wait status is one that results from our attaching to
     it or sending it a SIGSTOP for lwp_pool_stop_all.  We do not
     report these via lwp_pool_stop_all.

   Most of these states are combinations of various semi-independent
   factors, which we'll name and define here:

   - RUNNING / STOPPED / DEAD: These are the kernel states of the LWP:
     it's either running freely and could stop at any moment, is
     stopped but can be continued, or has died.

   - INTERESTING: this LWP has stopped or died with a wait status that
     has not yet been reported via lwp_pool_waitpid.  It is on the
     interesting LWP queue.

     This never applies to RUNNING LWPs: we never continue an
     INTERESTING LWP until we've reported its status.

     It always applies to DEAD LWPs.

   - STOP PENDING: we've sent this LWP a SIGSTOP, or attached to it,
     but we haven't yet received the boring WIFSTOPPED SIGSTOP status.

     This never applies to DEAD LWPs; the wait status that announces a
     LWP's death is always the last for that LWP.

   We could certainly represent these with independent bits or
   bitfields, but not all combinations are possible.  So instead, we
   assign each possible combination a distinct enum value, to make it
   easier to enumerate all the valid possibilities and be sure we've
   handled them.  */

enum lwp_state {

  /* An uninitialized LWP entry.  Only the lookup function itself,
     hash_find, creates entries in this state, and any function
     that calls that should put the entry in a meaningful state before
     returning.  */
  lwp_state_uninitialized,

  /* RUNNING.  This LWP is running --- last we knew.  It may have
     exited or been terminated by a signal, or it may have had a
     signal about to be delivered to it.  We won't know until we wait
     for it.  */
  lwp_state_running,

  /* STOPPED.  This LWP has stopped, and has no interesting status to
     report.  */
  lwp_state_stopped,

  /* STOPPED, INTERESTING.  This LWP has stopped with an interesting
     wait status, which we haven't yet reported to the user.  It is on
     the interesting LWP queue.  */
  lwp_state_stopped_interesting,

  /* DEAD, INTERESTING.  This LWP exited, or was killed by a signal.
     This LWP is on the interesting LWP queue.  Once we've reported it
     to the user, we'll delete it altogether.  */
  lwp_state_dead_interesting,

  /* RUNNING, STOP PENDING.  This LWP was running, and will eventually
     stop with a boring WIFSTOPPED SIGSTOP status, but may report an
     interesting status first.

     It's always safe to wait for an LWP in this state, so we do that
     as soon as possible; there shouldn't be any LWPs in this state
     between calls to public lwp_pool functions.  This is an
     internal-use state.  */
  lwp_state_running_stop_pending,

  /* STOPPED, STOP PENDING.  This LWP is stopped, and has no
     interesting status to report, but still has a boring status on
     the way.  After we report the status for a STOPPED, STOP PENDING,
     and INTERESTING LWP, this is the state it enters.

     See the note below on why this state is not avoidable.  */
  lwp_state_stopped_stop_pending,

  /* STOPPED, STOP PENDING, and INTERESTING.  This LWP has stopped with
     an interesting wait status.  We're also expecting a boring wait
     status from it.  */
  lwp_state_stopped_stop_pending_interesting,

};


/* Why we need lwp_state_stopped_stop_pending:

   I originally thought we could avoid having this state at all by
   simply always continuing STOPPED, STOP PENDING, INTERESTING LWPs
   in lwp_pool_waitpid as soon as we reported their wait status, and
   then waiting for them immediately, making them either STOPPED and
   un-INTERESTING, or STOPPED, STOP PENDING, and INTERESTING again.

   But the user has the right to call lwp_pool_continue_lwp on any LWP
   they've just gotten a wait status for --- and this simplification
   interferes with that.  First, note that you mustn't call
   continue_lwp on an interesting LWP: you might get yet another
   interesting wait status, and we don't want to queue up multiple
   interesting wait statuses per LWP --- the job is complex enough
   already.  Then, note that the proposed simplification means that
   lwp_pool_waitpid could return a status for some LWP, and have that
   LWP still be interesting.  If that happens, then you've got an LWP
   the user has the right to continue, but that can't actually be
   continued.

   I first tried to deal with this by having lwp_pool_continue_lwp
   simply do nothing if the user continues an interesting LWP.  After
   all, it's already in the interesting queue, so lwp_pool_waitpid
   will report it, and the user will be none the wiser.  But that's
   wrong: the user can specify a signal to deliver when they continue
   the LWP, and the only way signals are ever delivered to traced LWPs
   is via ptrace continue and single-step requests.  You can't use
   kill: that *generates* a signal, it doesn't *deliver* it.  You'd
   just get the signal back again via waitpid.  So if we don't
   actually continue the LWP with the user's signal, we've lost our
   only chance to deliver it.

   Clear as mud, no doubt.  I did my best.  */


struct lwp
{
  /* This lwp's PID.  */
  pid_t pid;

  /* The state this LWP is in.  */
  enum lwp_state state;

  /* If STATE is one of the lwp_state_*_interesting states, then this
     LWP is on the interesting LWP queue, headed by interesting_queue.

     If STATE is lwp_state_running_stop_pending, then this LWP is on
     the stopping LWP queue, stopping_queue.  (Note that
     stopping_queue is local to lwp_pool_stop_all; no LWP should be in
     that state by the time that function returns.  */
  struct lwp *prev, *next;

  /* If STATE is one of the lwp_state_*_interesting states, then
     STATUS is the interesting wait status.  */
  int status;

  /* Indicates the stepping status.  We must be prepared to step the
     given lwp upon continue since it's possible to get thread notification
     signals prior to a step actually occuring.  Receipt of a SIGTRAP is
     sufficient to clear this flag.  */
  int do_step;

  /* Indicates whether the lwp should be considered when continuing or
     stepping.  */
  int disabled;
};
 
  

/* The LWP hash table.  */

/* A hash table of all the live LWP's we know about.
   hash_population is the number of occupied entries in the table.

   hash_size is the total length of the table; it is always a power of
   two.  We resize the table to ensure that it is between 12.5% and
   50% occupied.  (Since the table's size is a power of two, resizing
   the table will always halve or double the populated ratio.  So
   there should be comfortably more than a factor of two between the
   maximum and minimum populations, for hysteresis.)

   The first slot we try is hash[PID % hash_size].  After C
   collisions, we try hash[(PID + C * STRIDE) % hash_size], where
   STRIDE is hash_size / 4 + 1.  The kernel assigns pids sequentially,
   so a STRIDE of 1, as many hash tables use, would make further
   collisions very likely.  But since hash_size is always a power of
   two, and hash_size / 4 + 1 is always odd, they are always
   relatively prime, so stepping by that many elements each time will
   eventually visit every table element.  A constant odd stride would
   be fine, but it's nice to have it scale with the overall population
   of the table.

   The table is an array of pointers to lwp's, rather than a direct
   array of lwp structures, so that pointers to lwp's don't become
   invalid when we rehash or delete entries.  */
static size_t hash_size, hash_population;
static struct lwp **hash;

/* We put the address of empty_lwp_slot into the hash table to
   represent deleted entries.  We do this so that we will not
   perturb the hash table while iterating through it.  */
static struct lwp empty_lwp_slot;

/* The minimum size for the hash table.  Small for testing.  */
enum { minimum_hash_size = 8 };


/* Return the hash slot for pid PID.  */
static int
hash_slot (pid_t pid, size_t size)
{
  return pid & (size - 1);
}


/* If there was a collision in SLOT, return the next slot.  */
static int
hash_next_slot (int slot, size_t size)
{
  int stride = size / 4 + 1;

  return (slot + stride) & (size - 1);
}


/* Return the earliest empty hash slot for PID.  */
static int
hash_empty_slot (pid_t pid)
{
  int slot = hash_slot (pid, hash_size);

  /* Since hash_next_slot will eventually visit every slot, and we
     know the table isn't full, this loop will terminate.  */
  while (hash[slot] && hash[slot] != &empty_lwp_slot)
    slot = hash_next_slot (slot, hash_size);

  return slot;
}


/* Return a new, empty hash table containing ELEMENTS elements.  This has
   no effect on the LWP pool's global variables.  */
static struct lwp **
make_hash_table (size_t elements)
{
  struct lwp **hash;
  size_t size = elements * sizeof (*hash);

  hash = malloc (size);
  memset (hash, 0, size);

  return hash;
}


/* Resize hash as needed to ensure that the table's population is
   between 12.5% and 50% of its size.  */
static void
resize_hash (void)
{
  struct lwp **new_hash;
  size_t new_hash_size;
  int new_hash_population; /* just for sanity checking */
  int i;

  /* Pick a new size.  */
  new_hash_size = hash_size;
  while (new_hash_size < hash_population * 2)
    new_hash_size *= 2;
  while (new_hash_size > minimum_hash_size
	 && new_hash_size > hash_population * 8)
    new_hash_size /= 2;

  /* We may have re-chosen the minimum table size.  */
  if (new_hash_size == hash_size)
    return;

  new_hash = make_hash_table (new_hash_size);
  new_hash_population = 0;

  /* Re-insert all the old lwp's in the new table.  */
  for (i = 0; i < hash_size; i++)
    if (hash[i] && hash[i] != &empty_lwp_slot)
      {
	struct lwp *l = hash[i];
	int new_slot = hash_slot (l->pid, new_hash_size);

	while (new_hash[new_slot])
	  new_slot = hash_next_slot (new_slot, new_hash_size);

	new_hash[new_slot] = l;
	new_hash_population++;
      }

  if (new_hash_population != hash_population)
    fprintf (stderr, "ERROR: rehashing changed population from %d to %d\n",
	     hash_population, new_hash_population);

  /* Free the old table, and drop in the new one.  */
  free (hash);
  hash = new_hash;
  hash_size = new_hash_size;
}


/* Find an existing hash table entry for LWP.  If there is none,
   create one in state lwp_state_uninitialized.  */
static struct lwp *
hash_find_1 (pid_t lwp, int create_p)
{
  int slot;
  struct lwp *l;

  /* Do we need to initialize the hash table?  */
  if (! hash)
    {
      hash_size = minimum_hash_size;
      hash = make_hash_table (hash_size);
      hash_population = 0;
    }

  for (slot = hash_slot (lwp, hash_size);
       hash[slot];
       slot = hash_next_slot (slot, hash_size))
    if (hash[slot]->pid == lwp)
      return hash[slot];

  if (!create_p)
    return NULL;

  /* There is no entry for this lwp.  Create one.  */
  l = malloc (sizeof (*l));
  l->pid = lwp;
  l->state = lwp_state_uninitialized;
  l->next = l->prev = NULL;
  l->status = 42;
  l->do_step = 0;
  l->disabled = 0;

  slot = hash_empty_slot (lwp);
  hash[slot] = l;
  hash_population++;

  /* Do we need to resize?  */
  if (hash_size < hash_population * 2)
    resize_hash ();

  return l;
}

static struct lwp *
hash_find (pid_t lwp)
{
  return hash_find_1 (lwp, 1);
}

static struct lwp *
hash_find_no_create (pid_t lwp)
{
  return hash_find_1 (lwp, 0);
}

/* Remove the LWP L from the pool.  This does not free L itself.  */
static void
hash_delete (struct lwp *l)
{
  int slot;

  for (slot = hash_slot (l->pid, hash_size);
       hash[slot];
       slot = hash_next_slot (slot, hash_size))
    if (hash[slot]->pid == l->pid)
      break;

  /* We shouldn't ever be asked to delete a 'struct lwp' that isn't in
     the table.  */
  assert (hash[slot]);

  /* There should be only one 'struct lwp' with a given PID.  */
  assert (hash[slot] == l);

#if 0
  /* Deleting from this kind of hash table is interesting, because of
     the way we handle collisions.

     For the sake of discussion, pretend that STRIDE is 1 (the
     reasoning is basically the same either way, but this has less
     hair).

     When we search for an LWP that hashes to slot S, because there
     may be collisions, the set of slots we'll actually search is the
     contiguous run of non-empty table entries that starts at S,
     heading towards higher indices (and possibly wrapping around at
     the end of the table).  When we find an empty table entry, we
     give up the search.

     When we delete an LWP, if we simply set its slot to zero, that
     could cause us to cut off later searches too early.  For example,
     if three LWP's all hash to slot S, and have been placed in slots
     S, S+1, and S+2, and we set slot S+1 to zero, then a search for
     the LWP at S+2 will start at S, and then stop at S+1 without ever
     seeing the right entry at S+2.

     Some implementations place a special "deleted" marker in the slot
     to let searches continue.  But then it's hard to ensure that the
     table doesn't get choked with deleted markers; and should deleted
     markers count towards the population for resizing purposes?  It's
     a mess.

     So after clearing a slot, we walk the remainder of the contiguous
     run of entries and re-hash them all.  If the hash function is
     doing a good job distributing entries across the table,
     contiguous runs should be short.  And it had better be good,
     because this is potentially quadratic.

     Of course, if we're going to resize the table, that removes all
     deleted elements, so we needn't bother with any of this.  */

  hash[slot] = NULL;
  hash_population--;

  if (hash_size > minimum_hash_size
      && hash_size > hash_population * 8)
    resize_hash ();
  else
    for (slot = hash_next_slot (slot, hash_size);
	 hash[slot];
	 slot = hash_next_slot (slot, hash_size))
      {
	struct lwp *refugee = hash[slot];

	hash[slot] = NULL;
	hash[hash_empty_slot (refugee->pid)] = refugee;
      }
#else
  /* The problem with the above (disabled) code above is that
     we may perturb the order of the entries when we rehash.  This
     is a serious problem when we're attempting to iterate through
     the hash table at the same time.

     The approach take here is to simply place the address of
     `empty_lwp_slot' into the deleted slot.  This slot will
     be reclaimed either when the hash table is resized or when
     it is encountered on insertion by traversing the collision
     chain.  */

  hash[slot] = &empty_lwp_slot;
  hash_population--;
#endif
}



/* Queue functions.  */ 

/* Insert L at the end of the queue headed by QUEUE.  */ 
static void
queue_enqueue (struct lwp *queue, struct lwp *l)
{
  assert (! l->next && ! l->prev);

  l->next = queue;
  l->prev = queue->prev;
  l->prev->next = l;
  l->next->prev = l;
}


/* If L is part of some queue, remove it.  */
static void
queue_delete (struct lwp *l)
{
  assert (l->next && l->prev);

  l->next->prev = l->prev;
  l->prev->next = l->next;
  l->next = l->prev = NULL;
}


/* Return non-zero if there is anything in QUEUE, zero otherwise.  */
static int
queue_non_empty (struct lwp *queue)
{
  return queue->next != queue;
}


/* Return the first LWP from QUEUE, but don't remove it.  If QUEUE is
   empty, return NULL.  */
static struct lwp *
queue_first (struct lwp *queue)
{
  struct lwp *l = queue->next;

  if (l != queue)
    return l;
  else
    return NULL;
}



/* Hashing LWP's, but with error checking and cleanup.  */


/* Add an entry for LWP to the pool and return it.  There should be no
   existing entry for LWP; if there is, clean it up.  The returned
   LWP's state is always lwp_state_uninitialized; the caller must
   initialize the LWP before returning.  */
static struct lwp *
hash_find_new (pid_t lwp)
{
  struct lwp *l = hash_find (lwp);

  if (l->state != lwp_state_uninitialized)
    {
      fprintf (stderr, "ERROR: new LWP %d already in table\n", (int) lwp);

      /* Remove ourselves from any queue we might be in.  */
      if (l->next)
	queue_delete (l);
    }

  l->state = lwp_state_uninitialized;

  return l;
}


/* Find an entry for an existing LWP, and return it.  If we have no
   existing entry for LWP, print an error message, but return the new,
   uninitialized entry anyway.  */
static struct lwp *
hash_find_known (pid_t lwp)
{
  struct lwp *l = hash_find (lwp);

  if (l->state == lwp_state_uninitialized)
    fprintf (stderr, "ERROR: unexpected lwp: %d\n", (int) lwp);

  return l;
}



/* Waiting.  */


/* The head of the queue of LWP's with interesting wait statuses.
   Only the prev and next members are meaningful.

   Every LWP in one of the lwp_state_*_interesting states should be on
   this queue.  If an LWP's state is lwp_state_dead_interesting, the
   LWP is not in the hash table any more.  */
static struct lwp interesting_queue
= { -1, 0, &interesting_queue, &interesting_queue, 42 };


static const char *
wait_status_str (int status)
{
  static char buf[100];

  if (WIFSTOPPED (status))
    sprintf (buf, "WIFSTOPPED (s) && WSTOPSIG (s) == %d (%s)",
	     WSTOPSIG (status), strsignal (WSTOPSIG (status)));
  else if (WIFEXITED (status))
    sprintf (buf, "WIFEXITED (s) && WEXITSTATUS (s) == %d",
	     WEXITSTATUS (status));
  else if (WIFSIGNALED (status))
    sprintf (buf, "WIFSIGNALED (s) && WTERMSIG (s) == %d (%s)%s",
	     WTERMSIG (status),
	     strsignal (WTERMSIG (status)),
	     WCOREDUMP (status) ? " && WCOREDUMP(s)" : "");
  else
    sprintf (buf, "%d (unrecognized status)", status);

  return buf;
}


static const char *
wait_flags_str (int flags)
{
  static const struct {
    int flag;
    const char *name;
  } flag_table[] = {
    { WNOHANG, "WNOHANG" },
    { WUNTRACED, "WUNTRACED" },
#ifdef __WCLONE
    { __WCLONE, "__WCLONE" },
#endif
#ifdef __WALL
    { __WALL, "__WALL" },
#endif
#ifdef __WNOTHREAD
    { __WNOTHREAD, "__WNOTHREAD" },
#endif
    { 0, 0 }
  };
  static char buf[100];
  int i;

  buf[0] = '\0';
  for (i = 0; flag_table[i].flag; i++)
    if (flags & flag_table[i].flag)
      {
	strcat (buf, flag_table[i].name);
	flags &= ~flag_table[i].flag;
	if (flags)
	  strcat (buf, " | ");
      }

  if (flags)
    sprintf (buf + strlen (buf), "0x%x", (unsigned) flags);

  if (buf[0] == '\0')
    return "0";
  else
    return buf;
}


static const char *
lwp_state_str (enum lwp_state state)
{
  switch (state)
    {
    case lwp_state_uninitialized:
      return "uninitialized";
    case lwp_state_running:
      return "running";
    case lwp_state_stopped:
      return "stopped";
    case lwp_state_stopped_interesting:
      return "stopped_interesting";
    case lwp_state_dead_interesting:
      return "dead_interesting";
    case lwp_state_running_stop_pending:
      return "running_stop_pending";
    case lwp_state_stopped_stop_pending:
      return "stopped_stop_pending";
    case lwp_state_stopped_stop_pending_interesting:
      return "stopped_stop_pending_interesting";
    default:
      {
	static char buf[100];
	sprintf (buf, "%d (unrecognized lwp_state)", state);
	return buf;
      }
    }
}


static void
debug_report_state_change (struct gdbserv *serv,
                           pid_t lwp,
			   enum lwp_state old,
			   enum lwp_state new)
{
  if (debug_lwp_pool && old != new)
    {
      fprintf (stderr,
	       "%32s -- %5d -> %s",
	       lwp_state_str (old), (int) lwp, lwp_state_str (new));
      if (new == lwp_state_stopped)
	fprintf (stderr, "    (at %#lx)", debug_get_pc (serv, lwp));
      fprintf (stderr, "\n");
    }
}

/* Remove (dead) LWP from the hash table and put it on the `interesting'
   queue.  */
static void
mark_lwp_as_dead_but_interesting (struct lwp *l)
{
  hash_delete (l);
  l->state = lwp_state_dead_interesting;
  if (l->next)
    queue_delete (l);
  queue_enqueue (&interesting_queue, l);
}

/* Wait for a status from the LWP L (or any LWP, if L is NULL),
   passing FLAGS to waitpid, and record the resulting wait status in
   the LWP pool appropriately.

   If no wait status was available (if FLAGS & WNOHANG), return zero.
   If we successfully processed some wait status, return 1.  If an
   error occurs, set errno and return -1.

   If waitpid returns an error, print a message to stderr.  */
static int
wait_and_handle (struct gdbserv *serv, struct lwp *l, int flags)
{
  int status;
  pid_t new_pid; 
  enum lwp_state old_state;
  
  /* We can only wait for LWP's that are running.  */
  if (l)
    assert (l->state == lwp_state_running
	    || l->state == lwp_state_running_stop_pending);

  /* This should be the only call to waitpid in this module, to ensure
     that we always keep each LWP's state up to date.  In fact, it
     should be the only call to waitpid used by any module using the
     LWP pool code at all.  */
  new_pid = waitpid (l ? l->pid : -1, &status, flags);

  if (debug_lwp_pool)
    {
      fprintf (stderr,
	       "lwp_pool: wait_and_handle: waitpid (%d, %s, %s) == %d\n",
	       l ? l->pid : -1,
	       (new_pid <= 0 ? "(unset)" : wait_status_str (status)),
	       wait_flags_str (flags),
	       new_pid);
    }

  if (new_pid == -1)
    {
      /* If we call fprintf, that'll wipe out the value of errno.  */
      int saved_errno = errno;

      fprintf (stderr, "ERROR: waitpid (%d) failed: %s\n",
	       l ? (int) l->pid : -1,
	       strerror (saved_errno));

      errno = saved_errno;
      return -1;
    }

  if (new_pid == 0)
    /* No status, so no LWP has changed state.  */
    return 0;

  if (l)
    {
      if (l->pid != new_pid)
	{
	  fprintf (stderr, "ERROR: waited for %d, but got %d\n",
		   l->pid, new_pid);
	  l = hash_find_known (new_pid);
	}
    }
  else
    l = hash_find_known (new_pid);

  old_state = l->state;
  
  l->status = status;

  if (WIFEXITED (status) || WIFSIGNALED (status))
    mark_lwp_as_dead_but_interesting (l);
  else
    {
      int stopsig;

      assert (WIFSTOPPED (status));
      
      stopsig = WSTOPSIG (status);

      if (stopsig == SIGTRAP)
	{
	  /* No longer stepping once a SIGTRAP is received.  */
	  l->do_step = 0;
	}

      switch (l->state)
	{
	case lwp_state_uninitialized:
	  /* Might as well clean it up.  */
	case lwp_state_running:
	  /* It stopped, but not because of anything we did, so it's
	     interesting even if it was a SIGSTOP.  */
	  l->state = lwp_state_stopped_interesting;
	  queue_enqueue (&interesting_queue, l);
	  break;

	case lwp_state_running_stop_pending:

	  /* If we were in stopping_queue, we're stopped now.  */
	  if (l->next)
	    queue_delete (l);

	  /* We are expecting a boring SIGSTOP.  Is this it?  */
	  if (stopsig == SIGSTOP)
	    l->state = lwp_state_stopped;
	  else
	    {
	      /* Report this status, but remember that we're still
		 expecting the boring SIGSTOP.  */
	      l->state = lwp_state_stopped_stop_pending_interesting;
	      queue_enqueue (&interesting_queue, l);
	    }
	  break;

	default:
	  /* The assert at top should prevent any other states from
	     showing up here.  */
	  fprintf (stderr, "ERROR: called waitpid on LWP %d in bad state %s\n",
		   (int) l->pid, lwp_state_str (l->state));
	  abort ();
	  break;
	}
    }

  debug_report_state_change (serv, l->pid, old_state, l->state);

  return 1;
}


/* Wait for a pending stop on the running LWP L.  Return non-zero if L
   ends up in an interesting state, or zero if L ends up in
   lwp_state_stopped.

   Whenever we have an LWP with no interesting status, but with a stop
   pending, we can always wait on it:

   - Since SIGCONT can't be blocked, caught, or ignored, the wait will
     always return immediately.  The process won't run amok.

   - Since the LWP is uninteresting to begin with, we'll end up with
     at most one interesting wait status to report; no need to queue
     up multiple statuses per LWP (which we'd rather not implement if
     we can avoid it).

   So, this function takes an LWP in lwp_state_running_stop_pending,
   and puts that LWP in either lwp_state_stopped (no stop pending) or
   some INTERESTING state.  It's really just wait_and_handle, with
   some error checking wrapped around it.  */
static int
check_stop_pending (struct gdbserv *serv, struct lwp *l)
{
  assert (l->state == lwp_state_running_stop_pending);

  wait_and_handle (serv, l, __WALL);

  switch (l->state)
    {
    case lwp_state_stopped:
      return 0;

    case lwp_state_stopped_stop_pending_interesting:
    case lwp_state_dead_interesting:
      return 1;

    case lwp_state_stopped_interesting:
      /* This state shouldn't happen: since there was a pending stop,
         a single waitpid on that LWP should have either gotten the
         SIGSTOP, yielding 'lwp_state_stopped', or something interesting,
         yielding 'lwp_state_stopped_stop_pending_interesting'.  */
    default:
      fprintf (stderr,
	       "ERROR: checking lwp %d for pending stop yielded "
	       "bad state %s\n",
	       (int) l->pid, lwp_state_str (l->state));
      hash_delete (l);
      if (l->next)
	queue_delete (l);
      free (l);
      return 0;
    }
}


pid_t
lwp_pool_waitpid (struct gdbserv *serv, pid_t pid, int *stat_loc, int options)
{
  struct lwp *l;
  enum lwp_state old_state;
  
  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_waitpid (%d, stat_loc, %s)\n",
	     (int) pid, wait_flags_str (options));

  /* Check that we're not being passed arguments that would be
     meaningful for the real waitpid, but that we can't handle.  */
  assert (pid == -1 || pid > 0);
  assert (! (options & ~WNOHANG));

  /* Do the wait, and choose an LWP to report on.  */
  if (pid == -1)
    {
      /* Handle wait statuses of any sort until something appears on
	 the interesting queue.  */
      while (! queue_non_empty (&interesting_queue))
	{
	  int result = wait_and_handle (serv, NULL, options | __WALL);

	  if (result <= 0)
	    return result;
	}

      l = queue_first (&interesting_queue);
    }
  else
    {
      /* Waiting for a status from a specific pid PID.  */
      l = hash_find_known (pid);

      /* We should only wait for known, running LWP's.  */
      assert (l->state == lwp_state_running
	      || l->state == lwp_state_running_stop_pending);

      /* Wait until this pid is no longer running.  */
      while (l->state == lwp_state_running
	     || l->state == lwp_state_running_stop_pending)
	{
	  int result = wait_and_handle (serv, l, options | __WALL);

	  if (result <= 0)
	    return result;
	}
    }

  /* Gather info from L early, in case we free it.  */
  pid = l->pid;
  old_state = l->state;
  if (stat_loc)
    *stat_loc = l->status;

  /* The INTERESTING states specifically mean that the LWP has a
     status which should be reported to the user, but that hasn't been
     yet.  Now we're about to report that status, so we need to mark
     interesting LWP's as uninteresting.  */
  switch (l->state)
    {
    case lwp_state_uninitialized:
    case lwp_state_running:
    case lwp_state_stopped:
    case lwp_state_stopped_stop_pending:
    case lwp_state_running_stop_pending:
      /* These are uninteresting states.  The waiting code above
	 should never have chosen an LWP in one of these states.  */
      fprintf (stderr,
	       "ERROR: %s: selected uninteresting LWP %d state %s\n",
	       __func__, l->pid, lwp_state_str (l->state));
      abort ();
      break;

    case lwp_state_stopped_interesting:
      /* Now that we've reported this wait status to the user, the LWP
	 is not interesting any more.  */
      l->state = lwp_state_stopped;
      queue_delete (l);
      debug_report_state_change (serv, l->pid, old_state, l->state);
      break;

    case lwp_state_dead_interesting:
      /* Once we've reported this status, we have washed our hands of
	 this LWP entirely.  */
      queue_delete (l);
      free (l);
      if (debug_lwp_pool)
	fprintf (stderr, 
		 "lwp_pool: %s: LWP %d state dead_interesting -> freed\n",
		 __func__, pid);
      break;

    case lwp_state_stopped_stop_pending_interesting:
      /* We're about to report this LWP's status, making it
	 uninteresting, but it's still got a stop pending.  */
      queue_delete (l);
      l->state = lwp_state_stopped_stop_pending;
      debug_report_state_change (serv, l->pid, old_state, l->state);
      break;

    default:
      fprintf (stderr, "ERROR: lwp %d in bad state: %s\n",
	       (int) l->pid, lwp_state_str (l->state));
      abort ();
      break;
    }

  return pid;
}



/* Stopping and continuing.  */


void
lwp_pool_stop_all (struct gdbserv *serv)
{
  int i;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_stop_all ()\n");

  /* The head of the queue of running LWP's that we are stopping.
     Only the prev and next members are meaningful.  */
  struct lwp stopping_queue;

  stopping_queue.next = stopping_queue.prev = &stopping_queue;

  /* First, put every LWP that's not already STOPPED or DEAD in a STOP
     PENDING state, and put them all on stopping_queue.  */ 
  for (i = 0; i < hash_size; i++)
    {
      struct lwp *l = hash[i];

      if (l && l != &empty_lwp_slot)
	{
	  enum lwp_state old_state = l->state;

	  switch (l->state)
	    {
	      /* There should never be 'uninitialized' entries left in
		 the table.  Whoever created them ought to have put them
		 in some meaningful state before returning.  */
	    case lwp_state_uninitialized:
	      assert (l->state != lwp_state_uninitialized);
	      break;

	    case lwp_state_running:
	      /* A 'no such process' error here indicates an NPTL thread
		 that has exited.  */
	      if (kill_lwp (l->pid, SIGSTOP) < 0)
		{
		  /* Thread has exited.  See if a status is available.  */
		  if (wait_and_handle (serv, l, WNOHANG) < 0)
		    {
		      /* Nope, it's truly gone without providing a status.
		         Put it on the interesting queue so that GDB is
			 notified that it's gone.  */
		      l->status = 0;
		      mark_lwp_as_dead_but_interesting (l);
		    }
		}
	      else
		{
		  l->state = lwp_state_running_stop_pending;
		  queue_enqueue (&stopping_queue, l);
		}

	      break;

	    case lwp_state_stopped:
            case lwp_state_stopped_stop_pending:
	    case lwp_state_stopped_interesting:
	    case lwp_state_dead_interesting:
	    case lwp_state_stopped_stop_pending_interesting:
	      /* Nothing needs to be done here.  */
	      break;

	    case lwp_state_running_stop_pending:
	      /* LWPs should never be in this state between calls to
		 public lwp_pool functions.  */
	      assert (l->state != lwp_state_running_stop_pending);
	      break;

	    default:
	      fprintf (stderr, "ERROR: lwp %d in bad state: %s\n",
		       (int) l->pid, lwp_state_str (l->state));
	      abort ();
	      break;
	    }

	  debug_report_state_change (serv, l->pid, old_state, l->state);
	}
    }

  /* Gather wait results until the stopping queue is empty.  */
  while (queue_non_empty (&stopping_queue))
    if (wait_and_handle (serv, NULL, __WALL) < 0)
      {
	fprintf (stderr, "ERROR: lwp_pool_stop_all wait failed: %s",
		 strerror (errno));
	return;
      }

  /* Now all LWPs should be stopped or dead.  But let's check.  */
  for (i = 0; i < hash_size; i++)
    {
      struct lwp *l = hash[i];
      if (l && l != &empty_lwp_slot)
	switch (l->state)
	  {
	  case lwp_state_uninitialized:
	    assert (l->state != lwp_state_uninitialized);
	    break;

	  case lwp_state_running:
	  case lwp_state_running_stop_pending:
	    fprintf (stderr,
		     "ERROR: lwp_pool_stop_all failed: LWP %d still running\n",
		     (int) l->pid);
	    break;

	  case lwp_state_stopped:
          case lwp_state_stopped_stop_pending:
	  case lwp_state_stopped_interesting:
	  case lwp_state_dead_interesting:
	  case lwp_state_stopped_stop_pending_interesting:
	    /* That's all as it should be.  */
	    break;

	  default:
	    fprintf (stderr, "ERROR: lwp %d in bad state: %s\n",
		     (int) l->pid, lwp_state_str (l->state));
	    abort ();
	    break;
	  }
    }
}

int
continue_or_step_lwp (struct gdbserv *serv, struct lwp *l, int sig)
{
  int status;
  if (l->do_step)
    status = singlestep_lwp (serv, l->pid, sig);
  else
    status = continue_lwp (l->pid, sig);

  return status;
}


void
lwp_pool_continue_all (struct gdbserv *serv)
{
  int i;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_continue_all ()\n");

  /* This loop makes every LWP either INTERESTING, or RUNNING.  */
  for (i = 0; i < hash_size; i++)
    {
      struct lwp *l = hash[i];

      if (l && l != &empty_lwp_slot && !l->disabled)
	{
	  enum lwp_state old_state = l->state;

	  switch (l->state)
	    {
	      /* There should never be 'uninitialized' entries left in
		 the table.  Whoever created them ought to have put them
		 in some meaningful state before returning.  */
	    case lwp_state_uninitialized:
	      assert (l->state != lwp_state_uninitialized);
	      break;

	    case lwp_state_running:
	      /* It's already running, so nothing needs to be done.  */
	      break;

	    case lwp_state_stopped:
	      if (continue_or_step_lwp (serv, l, 0) == 0)
		l->state = lwp_state_running;
	      break;

	    case lwp_state_stopped_interesting:
	    case lwp_state_dead_interesting:
	    case lwp_state_stopped_stop_pending_interesting:
	      /* We still have an unreported wait status here, so leave it
		 alone; we'll report it.  */
	      break;

	    case lwp_state_running_stop_pending:
	      /* There shouldn't be any LWPs in this state at this
		 point.  We should be calling check_stop_pending or
		 wait_and_handle as soon as we create them.  */
	      assert (l->state != lwp_state_running_stop_pending);
	      break;

            case lwp_state_stopped_stop_pending:
              /* Continue it, and then wait for the pending stop.
                 Since SIGSTOP cannot be blocked, caught, or ignored,
                 the wait will always return immediately; the LWP
                 won't run amok.  */
              if (continue_lwp (l->pid, 0) == 0)
                {
                  l->state = lwp_state_running_stop_pending;
                  if (check_stop_pending (serv, l) == 0)
                    {
                      if (continue_or_step_lwp (serv, l, 0) == 0)
                        l->state = lwp_state_running;
                    }
                }
              break;

	    default:
	      fprintf (stderr, "ERROR: lwp %d in bad state: %s\n", 
		       (int) l->pid, lwp_state_str (l->state));
	      abort ();
	      break;
	    }

	  debug_report_state_change (serv, l->pid, old_state, l->state);
	}
    }
}


int
lwp_pool_continue_lwp (struct gdbserv *serv, pid_t pid, int signal)
{
  struct lwp *l = hash_find_known (pid);
  enum lwp_state old_state = l->state;
  int result = 0;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_continue_lwp (%d, %d)\n",
	     (int) pid, signal);

  switch (l->state)
    {
    case lwp_state_uninitialized:
      assert (l->state != lwp_state_uninitialized);
      break;

      /* We should only be continuing LWPs that have reported a
         WIFSTOPPED status via lwp_pool_waitpid and have not been
         continued or singlestepped since.  */
    case lwp_state_running:
    case lwp_state_stopped_interesting:
    case lwp_state_dead_interesting:
    case lwp_state_running_stop_pending:
      fprintf (stderr, "ERROR: continuing LWP %d in unwaited state: %s\n",
               (int) l->pid, lwp_state_str (l->state));
      break;

    case lwp_state_stopped_stop_pending_interesting:
      if (debug_lwp_pool)
	fprintf (stderr, "WARNING: continuing LWP %d in unwaited state: %s\n",
		 (int) l->pid, lwp_state_str (l->state));
      break;

    case lwp_state_stopped:
      result = continue_or_step_lwp (serv, l, signal);
      if (result == 0)
        l->state = lwp_state_running;
      break;

    case lwp_state_stopped_stop_pending:
      /* Continue it, delivering the given signal, and then wait for
         the pending stop.  Since SIGSTOP cannot be blocked, caught,
         or ignored, the wait will always return immediately; the LWP
         won't run amok.

         We must deliver the signal with the first continue_lwp call;
         if check_stop_pending says the LWP has a new interesting
         status, then we'll never reach the second continue_lwp, and
         we'll lose our chance to deliver the signal.  */
      if (continue_lwp (l->pid, signal) == 0)
        {
          l->state = lwp_state_running_stop_pending;
          if (check_stop_pending (serv, l) == 0)
            {
              if (continue_or_step_lwp (serv, l, 0) == 0)
                l->state = lwp_state_running;
            }
        }
      break;

    default:
      fprintf (stderr, "ERROR: lwp %d in bad state: %s\n", 
               (int) l->pid, lwp_state_str (l->state));
      abort ();
      break;
    }

  debug_report_state_change (serv, l->pid, old_state, l->state);

  return result;
}


/* Clear the `do_step' flags for all LWPs in the hash table.  */

static void
clear_all_do_step_flags (void)
{
  int i;

  for (i = 0; i < hash_size; i++)
    {
      struct lwp *l = hash[i];

      if (l && l != &empty_lwp_slot)
	l->do_step = 0;
    }
}


int
lwp_pool_singlestep_lwp (struct gdbserv *serv, pid_t lwp, int signal)
{
  struct lwp *l = hash_find_known (lwp);
  enum lwp_state old_state = l->state;
  int result = 0;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_singlestep_lwp (%p, %d, %d)\n",
	     serv, (int) lwp, signal);

  /* Neither GDB nor the software singlestep code contained in RDA
     expect more than one LWP to be stepped simultaneously.  Clear the
     `do_step' flag in all LWPs.  The flag for the LWP that we're about
     to step will be set later on.  */
  clear_all_do_step_flags ();

  switch (l->state)
    {
    case lwp_state_uninitialized:
      assert (l->state != lwp_state_uninitialized);
      break;

      /* We should only be stepping LWPs that have reported a
         WIFSTOPPED status via lwp_pool_waitpid and have not been
         continued or singlestepped since.  */
    case lwp_state_running:
    case lwp_state_stopped_interesting:
    case lwp_state_dead_interesting:
    case lwp_state_running_stop_pending:
    case lwp_state_stopped_stop_pending_interesting:
      fprintf (stderr, "ERROR: stepping LWP %d in unwaited state: %s\n",
               (int) l->pid, lwp_state_str (l->state));
      break;

    case lwp_state_stopped:
      result = singlestep_lwp (serv, l->pid, signal);
      if (result == 0)
	{
	  l->state = lwp_state_running;
	  l->do_step = 1;
	}
      break;

    case lwp_state_stopped_stop_pending:
      /* Continue it, delivering the given signal, and then wait for
         the pending stop.  Since SIGSTOP cannot be blocked, caught,
         or ignored, the wait will always return immediately; the LWP
         won't run amok.

         We must deliver the signal with the continue_lwp call; if
         check_stop_pending says the LWP has a new interesting status,
         then we'll never reach the singlestep_lwp, and we'll lose our
         chance to deliver the signal at all.  */
      if (continue_lwp (l->pid, signal) == 0)
        {
          l->state = lwp_state_running_stop_pending;
          if (check_stop_pending (serv, l) == 0)
            {
              if (singlestep_lwp (serv, l->pid, 0) == 0)
		{
		  l->state = lwp_state_running;
		  l->do_step = 1;
		}
            }
        }
      break;

    default:
      fprintf (stderr, "ERROR: lwp %d in bad state: %s\n", 
               (int) l->pid, lwp_state_str (l->state));
      abort ();
      break;
    }

  debug_report_state_change (serv, l->pid, old_state, l->state);

  return result;
}



/* Adding new LWP's to the pool.  */

void
lwp_pool_new_stopped (pid_t pid)
{
  struct lwp *l = hash_find_new (pid);

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_new_stopped (%d)\n", (int) pid);

  l->state = lwp_state_stopped;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool: %s: new LWP %d state %s\n",
	     __func__, l->pid, lwp_state_str (l->state));
}


int
lwp_pool_attach (struct gdbserv *serv, pid_t pid)
{
  /* Are we already managing this LWP?  */
  struct lwp *l = hash_find (pid);

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_attach (%d)\n", (int) pid);

  if (l->state == lwp_state_uninitialized)
    {
      /* No, we really need to attach to it.  */
      int status = attach_lwp (pid);

      if (status)
	{
	  /* Forget about the lwp.  */
	  hash_delete (l);
	  free (l);
	  return status;
	}

      /* Since we attached to it, we'll get a SIGSTOP for this
	 eventually.  Wait for it now, to put it in either
	 lwp_state_stopped, or in some interesting state.  */
      l->state = lwp_state_running_stop_pending;

      if (debug_lwp_pool)
	fprintf (stderr, "lwp_pool: %s: new LWP %d state %s\n",
		 __func__, l->pid, lwp_state_str (l->state));

      check_stop_pending (serv, l);

      return 1;
    }
     
  return 0;
}

void
lwp_pool_disable_lwp (pid_t pid)
{
  struct lwp *l = hash_find_no_create (pid);

  if (l)
    {
      l->disabled = 1;

      if (debug_lwp_pool)
	fprintf (stderr, "lwp_pool_disable_lwp: disabling %d\n", pid);
    }
  else
    {
      if (debug_lwp_pool)
	fprintf (stderr, "lwp_pool_disable_lwp: pid %d not in pool\n", pid);
    }
}

void
lwp_pool_enable_lwp (pid_t pid)
{
  struct lwp *l = hash_find_no_create (pid);

  if (l)
    {
      l->disabled = 0;

      if (debug_lwp_pool)
	fprintf (stderr, "lwp_pool_enable_lwp: disabling %d\n", pid);
    }
  else
    {
      if (debug_lwp_pool)
	fprintf (stderr, "lwp_pool_enable_lwp: pid %d not in pool\n", pid);
    }
}
@


1.7
log
@	* lwp-pool.c (hash_find): Initialize `do_step' in the newly allocated
	lwp struct.
@
text
@d329 4
d366 5
d401 1
a401 1
  while (hash[slot])
d450 1
a450 1
    if (hash[i])
d476 1
a476 1
hash_find (pid_t lwp)
d495 3
d505 1
d507 1
d518 11
d549 1
d602 15
d1175 1
a1175 1
      if (l)
d1249 1
a1249 1
      if (l)
d1306 1
a1306 1
      if (l)
a1394 1
    case lwp_state_stopped_stop_pending_interesting:
d1399 6
d1456 1
a1456 1
      if (l)
d1600 38
@


1.6
log
@Don't allow more than one LWP to have `do_step' flag set at a time.
@
text
@d492 1
@


1.5
log
@Handle LWPs that have died without leaving a status.
@
text
@d1398 17
d1426 6
@


1.4
log
@Fix problem in which singlestep operations were apparently ignored.
@
text
@d799 11
d882 1
a882 9
    {
      /* Remove dead LWP's from the hash table, and put them in the
	 interesting queue.  */
      hash_delete (l);
      l->state = lwp_state_dead_interesting;
      if (l->next)
	queue_delete (l);
      queue_enqueue (&interesting_queue, l);
    }
d1149 18
a1166 3
	      kill_lwp (l->pid, SIGSTOP);
	      l->state = lwp_state_running_stop_pending;
	      queue_enqueue (&stopping_queue, l);
@


1.3
log
@Add diagnostic messages.
@
text
@d323 6
d888 6
d1220 12
d1264 1
a1264 1
	      if (continue_lwp (l->pid, 0) == 0)
d1292 1
a1292 1
                      if (continue_lwp (l->pid, 0) == 0)
d1341 1
a1341 1
      result = continue_lwp (l->pid, signal);
d1361 1
a1361 1
              if (continue_lwp (l->pid, 0) == 0)
d1412 4
a1415 1
        l->state = lwp_state_running;
d1434 4
a1437 1
                l->state = lwp_state_running;
@


1.2
log
@Add support for the Linux NPTL (New POSIX Thread Library) thread
implementation.
* arch.h, lwp-pool.c, lwp-pool.h, stock-breakpoints.c:
* stock-breakpoints.h: New files.
* gdbserv-thread-db.h (continue_lwp, singlestep_lwp, attach_lwp)
(stop_lwp): Move these ...
* lwp-ctrl.h: ... to here (new file).
* server.h (struct child_process): Include a pointer to the
architecture object, a breakpoint table, and a pointer to the
"focus thread".
* linux-target.c: Create architecture objects for the i386 and
FRV, and describe breakpoints for those architectures.
#include "arch.h".
[STOCK_BREAKPOINTS]: #include "stock-breakpoints.h".
(allocate_empty_arch): New function.
[X86_LINUX_TARGET] (stock_table_to_x86, x86_table_to_stock)
(stock_bp_to_x86, x86_bp_to_stock, x86_make_bp_table, x86_set_bp)
(x86_delete_bp, x86_bp_hit_p, x86_make_arch): New functions.
(MAKE_ARCH): #define to call x86_make_arch.
[X86_LINUX_TARGET] (stock_table_to_frv, frv_table_to_stock)
(stock_bp_to_frv, frv_bp_to_stock, frv_make_bp_table, frv_set_bp)
(frv_delete_bp, frv_bp_hit_p, frv_make_arch): New functions.
(MAKE_ARCH): #define to call frv_make_arch.
(linux_attach): If MAKE_ARCH is #defined, create an architecture
object for this process.  And if we have an architecture object,
make a breakpoint table.
* thread-db.c: #include <assert.h>, "arch.h", "lwp-ctrl.h", and
"lwp-pool.h".
(struct gdbserv_thread): Remove state-tracking flags 'attached',
'stopped', 'waited', and 'stepping'; all that state is handled
inside lwp-pool.c now.
(add_thread_to_list): Zero the entire newly allocated thread
structure.
(thread_list_lookup_by_lid): Prefer threads whose ti_lid values
are different from that of the main process.
(thread_db_state_str, thread_db_type_str): Don't include
formatting spaces here.
(thread_db_event_str): New function.
(thread_debug_name): New function.
(get_thread_signals): Return a value indicating success or failure.
(ignore_thread_signal): New function.
(using_thread_db_events, create_notification, death_notification)
(get_event_notification, set_event_breakpoint)
(insert_thread_db_event_breakpoints)
(delete_thread_db_event_breakpoints, request_thread_db_events)
(hit_thread_db_event_breakpoint, handle_thread_db_event): New
variables and functions, for using libthread_db's event interface.
(thread_db_open): Gather error-handling code next to the operation
that might fail.  Decide here whether to use the signal-based
debugging interface, or the event-based debugging interface.
(stop_thread, stop_all_threads, struct event_list, pending_events)
(pending_events_listsize, pending_events_top, add_pending_event)
(select_pending_event, send_pending_signals, wait_all_threads):
Deleted; we now use lwp-pool.c for all this.
(attach_thread, continue_thread, continue_all_threads)
(singlestep_thread, thread_db_singlestep_program)
(thread_db_continue_thread, thread_db_singlestep_thread): Use the
lwp-pool.c functions, instead of calling the lwp manipulation
functions directly and managing state here.  Keep track of the
focus thread.
(find_new_threads_callback): Always call attach_thread.  If we're
using libthread_db events, enable event reporting for the new
thread.
(update_thread_list): Take the current child process as an
argument; if the current focus thread disappears, clear the
process's focus_thread pointer.
(thread_db_thread_next): Pass the current child process to
update_thread_list.
(thread_db_thread_info): Reformat thread descriptions.  Mark
threads whose pid is equal to the process ID.
(thread_db_check_child_state): Use the lwp-pool functions, instead
of calling waitpid and the old stop-all-threads functions.  If we
have a focus thread, only check for a status on that thread.
Check for libthread_db events.  Use ignore_thread_signal, instead
of writing it out.
(thread_db_break_program): New function.
(thread_db_attach): Register it as the target's 'break_program'
method.  Always preload the symbol list with the names we'll need
for the signal-based interface, even if we have td_symbol_list.
Use lwp_pool_new_stopped to register the initial thread.  Clear
the focus thread.
* ptrace-target.c: #define _GNU_SOURCE and #include <sys/types.h>
and <linux/unistd.h>, to get declarations for the functions we
need.
#include "lwp-ctrl.h".
(continue_lwp, singlestep_lwp, attach_lwp): Remove
unnecessary 'extern' keywords.  Move pre-function comments to
lwp-ctrl.h.  Take an ordinary 'pid_t', not an 'lwpid_t', since
we're calling ptrace / tkill, and that's what they expect; rename
arguments accordingly.  Preserve value of errno across calls to
fprintf when reporting errors.
(kill_lwp): All the above, and use tkill system call if available.
* configure.in: Whenever we include thread-db.o in TARGET_MODULES,
also include lwp-pool.o.
On i386 and FRV Linux, use the stock-breakpoints module.
* config.in: Add template for STOCK_BREAKPOINTS.
* Makefile.am (EXTRA_rda_SOURCES): Include stock-breakpoints.c.
* Makefile.in, aclocal.m4, configure: Regenerated.
@
text
@d40 3
a42 1
static int debug_lwp_pool = 0;
d777 2
a778 1
debug_report_state_change (pid_t lwp,
d783 8
a790 3
    fprintf (stderr,
	     "%32s -- %5d -> %-32s\n",
	     lwp_state_str (old), (int) lwp, lwp_state_str (new));
d804 1
a804 1
wait_and_handle (struct lwp *l, int flags)
d921 1
a921 1
  debug_report_state_change (l->pid, old_state, l->state);
d947 1
a947 1
check_stop_pending (struct lwp *l)
d951 1
a951 1
  wait_and_handle (l, __WALL);
d982 1
a982 1
lwp_pool_waitpid (pid_t pid, int *stat_loc, int options)
d1003 1
a1003 1
	  int result = wait_and_handle (NULL, options | __WALL);
d1024 1
a1024 1
	  int result = wait_and_handle (l, options | __WALL);
d1061 1
a1061 1
      debug_report_state_change (l->pid, old_state, l->state);
d1080 1
a1080 1
      debug_report_state_change (l->pid, old_state, l->state);
d1099 1
a1099 1
lwp_pool_stop_all (void)
d1160 1
a1160 1
	  debug_report_state_change (l->pid, old_state, l->state);
d1166 1
a1166 1
    if (wait_and_handle (NULL, __WALL) < 0)
d1210 1
a1210 1
lwp_pool_continue_all (void)
d1266 1
a1266 1
                  if (check_stop_pending (l) == 0)
d1281 1
a1281 1
	  debug_report_state_change (l->pid, old_state, l->state);
d1288 1
a1288 1
lwp_pool_continue_lwp (pid_t pid, int signal)
d1335 1
a1335 1
          if (check_stop_pending (l) == 0)
d1350 1
a1350 1
  debug_report_state_change (l->pid, old_state, l->state);
d1404 1
a1404 1
          if (check_stop_pending (l) == 0)
d1419 1
a1419 1
  debug_report_state_change (l->pid, old_state, l->state);
d1445 1
a1445 1
lwp_pool_attach (pid_t pid)
d1475 1
a1475 1
      check_stop_pending (l);
@


1.1
log
@file lwp-pool.c was initially added on branch jimb-rda-nptl-branch.
@
text
@d1 1473
@


1.1.2.1
log
@Separate management of kernel-level LWPs from that of libpthread /
libthread_db-level threads.
* lwp-pool.c, lwp-pool.h: New files.
* thread-db.c: #include "lwp-ctrl.h" and "lwp-pool.h".
(struct gdbserv_thread): Delete members 'attached', 'stopped',
'waited', and 'stepping'.  This structure is now just a
'td_thrinfo_t' and a list link.  Describe some quirks in the
meanings of certain 'ti' fields.
(thread_list_lookup_by_lid): Move later in file, so we can use
information directly from our proc handle.  Be skeptical of ZOMBIE
or UNKNOWN threads whose LWP ID is equal to the PID in the proc
handle.
(thread_debug_name): Move later in file, so we can use
thread_db_state_str.
(attach_thread): Use lwp pool functions to attach.  Attach to
zombies.  When using signal-based communication, send the thread
the restart signal immediately.
(find_new_threads_callback): Go ahead and attach to all threads.
The LWP pool functions tolerate attaching to a given LWP more than
once.
(update_thread_list): Take the process as an argument.  If the
focus thread has disappeared, set process->focus_thread to NULL.
(thread_db_thread_next): Pass the process to update_thread_list.
(stop_thread, stop_all_threads, add_pending_event,
delete_pending_event, select_pending_event, send_pending_signals,
wait_all_threads, continue_all_threads): Deleted.
(handle_thread_db_event): Renamed from handle_thread_db_events.
Take the process structure as an argument, and check only for a
thread-db event notification from process->event_thread.  Use LWP
pool functions.
(continue_thread, singlestep_thread): Use LWP pool functions.
(thread_db_continue_program, thread_db_singlestep_program,
thread_db_continue_thread, thread_db_singlestep_thread): Use LWP
pool functions, and update process->focus_thread appropriately.
(thread_db_check_child_state): Use the LWP pool functions.  Rather
than stopping all LWP's, choosing the most interesting events, and
then arranging to re-create all the other wait statuses we got,
just pick the first event we get from lwp_pool_waitpid (either on
the focus thread, if there is one, or on any thread) and report
that.  Use the new handle_thread_db_event function.
(struct event_list, pending_events, pending_events_listsize,
pending_events_top): Deleted; replaced by LWP pool code.
(thread_db_attach): Tell the LWP pool about the PID we're
attaching to.  Clear the focus thread.
* server.h (struct process): New member: 'focus_thread'.
* gdbserv-thread-db.h (continue_lwp, singlestep_lwp, attach_lwp,
stop_lwp): Move declarations from here...
* lwp-ctrl.h: ... to here.  New file.
(kill_lwp): Renamed from stop_lwp; allow caller to specify any
signal.
* ptrace-target.c: #include "lwp-ctrl.h".
(continue_lwp, singlestep_lwp, attach_lwp, stop_lwp): Move
function comments to lwp-ctrl.h, and expand.
* configure.in: Whenever we select 'thread-db.o', select
'lwp-pool.o' as well.
* configure: Regenerated.

* thread-db.c (thread_db_check_child_state): Remove extraneous
call to handle_waitstatus.  Remove extra check for exited main
thread.

* thread-db.c (thread_db_thread_info): List the type and state
before the PID, and mention whether the LWP's PID is equal to that
of the main thread, since ZOMBIE and UNKNOWN threads whose LWP's
PID is equal are probably actually exited threads.

* thread-db.c (add_thread_to_list): Zero out entire structure.

* thread-db.c (thread_db_state_str, thread_db_type_str): Remove
spaces from names; we don't always want them, and the caller can
use printf formatting directives to arrange things as they please.

* ptrace-target.c (continue_lwp, singlestep_lwp, attach_lwp,
stop_lwp): Change arguments from 'lwpid_t' to 'pid_t'.  lwpid_t is
strictly a thread-db type; these are functions that use system
calls, which all expect pid_t.  Rename arguments from 'lwpid' to
'pid'.

* ptrace-target.c: #define _GNU_SOURCE to get declaration for
strsignal.
(kill_lwp): Enhance error reporting.
@
text
@a0 1468
/* lwp-pool.c --- implementation of a stoppable, waitable LWP pool.

   Copyright 2004 Red Hat, Inc.

   This file is part of RDA, the Red Hat Debug Agent (and library).

   This program is free software; you can redistribute it and/or modify
   it under the terms of the GNU General Public License as published by
   the Free Software Foundation; either version 2 of the License, or
   (at your option) any later version.

   This program is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You should have received a copy of the GNU General Public License
   along with this program; if not, write to the Free Software
   Foundation, Inc., 59 Temple Place - Suite 330,
   Boston, MA 02111-1307, USA.
   
   Alternative licenses for RDA may be arranged by contacting Red Hat,
   Inc.  */

#include "config.h"

#define _GNU_SOURCE /* for strerror */

#include <assert.h>
#include <stdlib.h>
#include <string.h>
#include <stdio.h>
#include <errno.h>
#include <sys/types.h>
#include <sys/wait.h>

#include "lwp-pool.h"
#include "lwp-ctrl.h"

static int debug_lwp_pool = 1;


/* THE LIFETIME OF A TRACED LWP

   POSIX uses these terms in talking about signals:

   - To "generate" a signal is to call kill or raise, divide by zero,
     etc.

   - To "deliver" a signal is to do whatever that signal's designated
     action is: ignore it, enter a signal handler, terminate the
     process, or stop the process.

   - To "accept" a signal is to have 'sigwait' or a similar function
     select and return the signal.

   - A signal is "pending" between the time it is generated and the
     time it is delivered.

   So, here is the life cycle of a traced LWP:

   - It is created by fork or vfork and does a PTRACE_TRACEME.  The
     PTRACE_TRACEME makes it a traced, running LWP.  When a traced LWP
     does an exec, it gets a SIGTRAP before executing the first
     instruction in the new process image, so the LWP will then stop.

     Or, we attach to it with a PTRACE_ATTACH.  This sends a SIGSTOP
     to the LWP, so it will stop.

   - While a traced LWP is stopped, we can read and write its
     registers and memory.  We can also send it signals; they become
     pending on the LWP, and are not delivered or accepted until it is
     continued.

   - A stopped LWP can be set running again in one of two ways:

     + by doing a PTRACE_CONT, PTRACE_SINGLESTEP, or PTRACE_SYSCALL; or

     + by sending it a SIGCONT.

     The ptrace requests all let you specify a signal to be delivered to
     the process.

     Sending a SIGCONT clears any pending SIGSTOPs; PTRACE_CONT and
     PTRACE_SINGLESTEP don't have that side effect.

     (Sending an LWP a SIGKILL via the 'kill' or 'tkill' system calls
     acts like sending it a SIGKILL followed by a SIGCONT.)

   - A running LWP may exit or be terminated by a signal at any time,
     so accessing its memory or registers or sending it a signal is
     always a race.

   - waitpid will eventually return a status S for a continued LWP:

     + If WIFEXITED (S) or WIFSIGNALED (S), the LWP no longer exists.
     
     + IF WIFSTOPPED (S), the LWP is stopped again, because some
       signal WSTOPSIG (S) was about to be delivered to it.  Here we
       go back to the second step.

       Note that the signal WSTOPSIG (S) has not yet been delivered to
       the process, and is no longer pending on the process.  Only
       signals passed to the ptrace requests get delivered.  In
       effect, the debugger gets to intercept signals before they are
       delivered, and decide whether to pass them through or not.
       (The exception is SIGKILL: that always produces a WIFSIGNALED
       wait status, and terminates the process.)

   So, to put all that together:

   - A traced LWP goes back and forth from running to stopped, until
     eventually it goes from running to exited or killed.

   - Running->stopped transitions are always signal deliveries, yielding
     WIFSTOPPED wait statuses.

   - Stopping->running transitions are generally due to ptrace
     requests by the debugger.  (The debugger could send signals, but
     that's messy.)

   - Running->exited transitions are due to, duh, the LWP exiting.

   - Running->killed transitions are due to a signal being delivered
     to the LWP that is neither ignored nor caught.


   Under NPTL, this life cycle is a bit different: LWPs simply exit,
   without creating a zombie; they produce no wait status.  The NPTL
   libthread_db generates a TD_DEATH event for them, but at the kernel
   level the only indication that they're gone is that the tkill
   system call fails with ESRCH ("No such process").

   Under LinuxThreads, LWPs remain zombie processes until they're
   waited for.  Attempts to send them signals while zombies have no
   effect, but return no error.


   STOPPING A PROCESS

   The major challenge here is implementing the lwp_pool_stop_all
   function.  The only way to stop a running LWP is to send it a
   SIGSTOP, and then wait for a status acknowledging the stop.  But as
   explained above, a running LWP could stop at any time of its own
   accord, so sending it a SIGSTOP is always a race.  By the time you
   call waitpid, you don't know whether you'll get a status for the
   SIGSTOP you just sent, or for something else: some other signal, an
   exit, or a termination by signal.

   If the LWP turns out to have exited or died, then that's pretty
   easy to handle.  Your attempt to send a SIGSTOP will get an error,
   and then you'll get a wait status for the termination.  A
   termination status is always the last status you'll get from wait
   for that LWP, so there'll be no further record of your SIGSTOP.

   If the LWP was about to have some other signal delivered to it,
   then the next wait will return a WIFSTOPPED status for that signal;
   we'll have to continue the LWP and wait again until we get the
   status for our SIGSTOP.  The kernel forgets about any signals the
   LWP has received once it has reported them to us, so it's up to us
   to keep track of them and report them via lwp_pool_waitpid.  */



/* The LWP structure.  */

/* The states an LWP we're managing might be in.

   For the purposes of these states, we classify wait statuses as
   follows:

   - An "interesting" wait status is one that isn't a result of us
     attaching to the LWP or sending it a SIGSTOP for
     lwp_pool_stop_all.  It indicates something that happened to the
     LWP other than as a result of this code's fiddling with it.  We
     report all interesting wait statuses via lwp_pool_waitpid.

   - A "boring" wait status is one that results from our attaching to
     it or sending it a SIGSTOP for lwp_pool_stop_all.  We do not
     report these via lwp_pool_stop_all.

   Most of these states are combinations of various semi-independent
   factors, which we'll name and define here:

   - RUNNING / STOPPED / DEAD: These are the kernel states of the LWP:
     it's either running freely and could stop at any moment, is
     stopped but can be continued, or has died.

   - INTERESTING: this LWP has stopped or died with a wait status that
     has not yet been reported via lwp_pool_waitpid.  It is on the
     interesting LWP queue.

     This never applies to RUNNING LWPs: we never continue an
     INTERESTING LWP until we've reported its status.

     It always applies to DEAD LWPs.

   - STOP PENDING: we've sent this LWP a SIGSTOP, or attached to it,
     but we haven't yet received the boring WIFSTOPPED SIGSTOP status.

     This never applies to DEAD LWPs; the wait status that announces a
     LWP's death is always the last for that LWP.

     There's nothing wrong with having STOPPED, un-INTERESTING, and
     STOP PENDING LWP's, but it turns out that we can always just
     continue the thread and wait immediately for it, making such a
     combination unnecessary.

     We could do something similar and eliminate the RUNNING, STOP
     PENDING state, but that state turns out to be handy for error
     checking.

   We could certainly represent these with independent bits or
   bitfields, but not all combinations are possible.  So instead, we
   assign each possible combination a distinct enum value, to make it
   easier to enumerate all the valid possibilities and be sure we've
   handled them.  */

enum lwp_state {

  /* An uninitialized LWP entry.  Only the lookup function itself,
     hash_find, creates entries in this state, and any function
     that calls that should put the entry in a meaningful state before
     returning.  */
  lwp_state_uninitialized,

  /* RUNNING.  This LWP is running --- last we knew.  It may have
     exited or been terminated by a signal, or it may have had a
     signal about to be delivered to it.  We won't know until we wait
     for it.  */
  lwp_state_running,

  /* STOPPED.  This LWP has stopped, and has no interesting status to
     report.  */
  lwp_state_stopped,

  /* STOPPED, INTERESTING.  This LWP has stopped with an interesting
     wait status, which we haven't yet reported to the user.  It is on
     the interesting LWP queue.  */
  lwp_state_stopped_interesting,

  /* DEAD, INTERESTING.  This LWP exited, or was killed by a signal.
     This LWP is on the interesting LWP queue.  Once we've reported it
     to the user, we'll delete it altogether.  */
  lwp_state_dead_interesting,

  /* RUNNING, STOP PENDING.  This LWP was running, and will eventually
     stop with a boring WIFSTOPPED SIGSTOP status, but may report an
     interesting status first.

     It's always safe to wait for a thread in this state, so we do
     that as soon as possible; there shouldn't be any threads in this
     state between calls to public lwp_pool functions.  This is an
     internal-use state.  */
  lwp_state_running_stop_pending,

  /* STOPPED, STOP PENDING, and INTERESTING.  This LWP has stopped with
     an interesting wait status.  We're also expecting a boring wait
     status from it.  */
  lwp_state_stopped_stop_pending_interesting,

};


/* The thread_db death state.  See the descriptions of the
   lwp_pool_thread_db_* functions in lwp-pool.h.  */
enum death_state {

  /* We've received no indication that this thread will exit.  */
  death_state_running,

  /* We've received a TD_DEATH event for this thread, but it hasn't
     completed its event notification yet.  */
  death_state_event_received,

  /* We've received a TD_DEATH event for this thread, and it has
     completed its event notification; when we continue it next, we
     will delete it from the hash table and forget about it
     entirely.  */
  death_state_delete_when_continued
};


struct lwp
{
  /* This lwp's PID.  */
  pid_t pid;

  /* The state this LWP is in.  */
  enum lwp_state state;

  /* Its thread_db death notification state.  */
  enum death_state death_state;

  /* If STATE is one of the lwp_state_*_interesting states, then this
     LWP is on the interesting LWP queue, headed by interesting_queue.

     If STATE is lwp_state_running_stop_pending, then this LWP is on
     the stopping LWP queue, stopping_queue.  (Note that
     stopping_queue is local to lwp_pool_stop_all; no thread should be
     in that state by the time that function returns.  */
  struct lwp *prev, *next;

  /* If STATE is one of the lwp_state_*_interesting states, then
     STATUS is the interesting wait status.  */
  int status;
};
 
  

/* The LWP hash table.  */

/* A hash table of all the live LWP's we know about.
   hash_population is the number of occupied entries in the table.

   hash_size is the total length of the table; it is always a power of
   two.  We resize the table to ensure that it is between 12.5% and
   50% occupied.  (Since the table's size is a power of two, resizing
   the table will always halve or double the populated ratio.  So
   there should be comfortably more than a factor of two between the
   maximum and minimum populations, for hysteresis.)

   The first slot we try is hash[PID % hash_size].  After C
   collisions, we try hash[(PID + C * STRIDE) % hash_size], where
   STRIDE is hash_size / 4 + 1.  The kernel assigns pids sequentially,
   so a STRIDE of 1, as many hash tables use, would make further
   collisions very likely.  But since hash_size is always a power of
   two, and hash_size / 4 + 1 is always odd, they are always
   relatively prime, so stepping by that many elements each time will
   eventually visit every table element.  A constant odd stride would
   be fine, but it's nice to have it scale with the overall population
   of the table.

   The table is an array of pointers to lwp's, rather than a direct
   array of lwp structures, so that pointers to lwp's don't become
   invalid when we rehash or delete entries.  */
static size_t hash_size, hash_population;
static struct lwp **hash;

/* The minimum size for the hash table.  Small for testing.  */
enum { minimum_hash_size = 8 };


/* Return the hash slot for pid PID.  */
static int
hash_slot (pid_t pid, size_t size)
{
  return pid & (size - 1);
}


/* If there was a collision in SLOT, return the next slot.  */
static int
hash_next_slot (int slot, size_t size)
{
  int stride = size / 4 + 1;

  return (slot + stride) & (size - 1);
}


/* Return the earliest empty hash slot for PID.  */
static int
hash_empty_slot (pid_t pid)
{
  int slot = hash_slot (pid, hash_size);

  /* Since hash_next_slot will eventually visit every slot, and we
     know the table isn't full, this loop will terminate.  */
  while (hash[slot])
    slot = hash_next_slot (slot, hash_size);

  return slot;
}


/* Return a new, empty hash table containing ELEMENTS elements.  This has
   no effect on the LWP pool's global variables.  */
static struct lwp **
make_hash_table (size_t elements)
{
  struct lwp **hash;
  size_t size = elements * sizeof (*hash);

  hash = malloc (size);
  memset (hash, 0, size);

  return hash;
}


/* Resize hash as needed to ensure that the table's population is
   between 12.5% and 50% of its size.  */
static void
resize_hash (void)
{
  struct lwp **new_hash;
  size_t new_hash_size;
  int new_hash_population; /* just for sanity checking */
  int i;

  /* Pick a new size.  */
  new_hash_size = hash_size;
  while (new_hash_size < hash_population * 2)
    new_hash_size *= 2;
  while (new_hash_size > minimum_hash_size
	 && new_hash_size > hash_population * 8)
    new_hash_size /= 2;

  /* We may have re-chosen the minimum table size.  */
  if (new_hash_size == hash_size)
    return;

  new_hash = make_hash_table (new_hash_size);
  new_hash_population = 0;

  /* Re-insert all the old lwp's in the new table.  */
  for (i = 0; i < hash_size; i++)
    if (hash[i])
      {
	struct lwp *l = hash[i];
	int new_slot = hash_slot (l->pid, new_hash_size);

	while (new_hash[new_slot])
	  new_slot = hash_next_slot (new_slot, new_hash_size);

	new_hash[new_slot] = l;
	new_hash_population++;
      }

  if (new_hash_population != hash_population)
    fprintf (stderr, "ERROR: rehashing changed population from %d to %d\n",
	     hash_population, new_hash_population);

  /* Free the old table, and drop in the new one.  */
  free (hash);
  hash = new_hash;
  hash_size = new_hash_size;
}


/* Find an existing hash table entry for LWP.  If there is none,
   create one in state lwp_state_uninitialized.  */
static struct lwp *
hash_find (pid_t lwp)
{
  int slot;
  struct lwp *l;

  /* Do we need to initialize the hash table?  */
  if (! hash)
    {
      hash_size = minimum_hash_size;
      hash = make_hash_table (hash_size);
      hash_population = 0;
    }

  for (slot = hash_slot (lwp, hash_size);
       hash[slot];
       slot = hash_next_slot (slot, hash_size))
    if (hash[slot]->pid == lwp)
      return hash[slot];

  /* There is no entry for this lwp.  Create one.  */
  l = malloc (sizeof (*l));
  l->pid = lwp;
  l->state = lwp_state_uninitialized;
  l->death_state = 0;
  l->next = l->prev = NULL;
  l->status = 42;

  hash[slot] = l;
  hash_population++;

  /* Do we need to resize?  */
  if (hash_size < hash_population * 2)
    resize_hash ();

  return l;
}


/* Remove the LWP L from the pool.  This does not free L itself.  */
static void
hash_delete (struct lwp *l)
{
  int slot;

  for (slot = hash_slot (l->pid, hash_size);
       hash[slot];
       slot = hash_next_slot (slot, hash_size))
    if (hash[slot]->pid == l->pid)
      break;

  /* We shouldn't ever be asked to delete a 'struct lwp' that isn't in
     the table.  */
  assert (hash[slot]);

  /* There should be only one 'struct lwp' with a given PID.  */
  assert (hash[slot] == l);

  /* Deleting from this kind of hash table is interesting, because of
     the way we handle collisions.

     For the sake of discussion, pretend that STRIDE is 1 (the
     reasoning is basically the same either way, but this has less
     hair).

     When we search for an LWP that hashes to slot S, because there
     may be collisions, the set of slots we'll actually search is the
     contiguous run of non-empty table entries that starts at S,
     heading towards higher indices (and possibly wrapping around at
     the end of the table).  When we find an empty table entry, we
     give up the search.

     When we delete an LWP, if we simply set its slot to zero, that
     could cause us to cut off later searches too early.  For example,
     if three LWP's all hash to slot S, and have been placed in slots
     S, S+1, and S+2, and we set slot S+1 to zero, then a search for
     the LWP at S+2 will start at S, and then stop at S+1 without ever
     seeing the right entry at S+2.

     Some implementations place a special "deleted" marker in the slot
     to let searches continue.  But then it's hard to ensure that the
     table doesn't get choked with deleted markers; and should deleted
     markers count towards the population for resizing purposes?  It's
     a mess.

     So after clearing a slot, we walk the remainder of the contiguous
     run of entries and re-hash them all.  If the hash function is
     doing a good job distributing entries across the table,
     contiguous runs should be short.  And it had better be good,
     because this is potentially quadratic.

     Of course, if we're going to resize the table, that removes all
     deleted elements, so we needn't bother with any of this.  */

  hash[slot] = NULL;
  hash_population--;

  if (hash_size > minimum_hash_size
      && hash_size > hash_population * 8)
    resize_hash ();
  else
    for (slot = hash_next_slot (slot, hash_size);
	 hash[slot];
	 slot = hash_next_slot (slot, hash_size))
      {
	struct lwp *refugee = hash[slot];

	hash[slot] = NULL;
	hash[hash_empty_slot (refugee->pid)] = refugee;
      }
}



/* Queue functions.  */ 

/* Insert L at the end of the queue headed by QUEUE.  */ 
static void
queue_enqueue (struct lwp *queue, struct lwp *l)
{
  assert (! l->next && ! l->prev);

  l->next = queue;
  l->prev = queue->prev;
  l->prev->next = l;
  l->next->prev = l;
}


/* If L is part of some queue, remove it.  */
static void
queue_delete (struct lwp *l)
{
  assert (l->next && l->prev);

  l->next->prev = l->prev;
  l->prev->next = l->next;
  l->next = l->prev = NULL;
}


/* Return non-zero if there is anything in QUEUE, zero otherwise.  */
static int
queue_non_empty (struct lwp *queue)
{
  return queue->next != queue;
}


/* Return the first LWP from QUEUE, but don't remove it.  If QUEUE is
   empty, return NULL.  */
static struct lwp *
queue_first (struct lwp *queue)
{
  struct lwp *l = queue->next;

  if (l != queue)
    return l;
  else
    return NULL;
}



/* Hashing LWP's, but with error checking and cleanup.  */


/* Add an entry for LWP to the pool and return it.  There should be no
   existing entry for LWP; if there is, clean it up.  The returned
   LWP's state is always lwp_state_uninitialized; the caller must
   initialize the LWP before returning.  */
static struct lwp *
hash_find_new (pid_t lwp)
{
  struct lwp *l = hash_find (lwp);

  if (l->state != lwp_state_uninitialized)
    {
      fprintf (stderr, "ERROR: new LWP %d already in table\n", (int) lwp);

      /* Remove ourselves from any queue we might be in.  */
      if (l->next)
	queue_delete (l);
    }

  l->state = lwp_state_uninitialized;

  return l;
}


/* Find an entry for an existing LWP, and return it.  If we have no
   existing entry for LWP, print an error message, but return the new,
   uninitialized entry anyway.  */
static struct lwp *
hash_find_known (pid_t lwp)
{
  struct lwp *l = hash_find (lwp);

  if (l->state == lwp_state_uninitialized)
    fprintf (stderr, "ERROR: unexpected lwp: %d\n", (int) lwp);

  return l;
}



/* Waiting.  */


/* The head of the queue of LWP's with interesting wait statuses.
   Only the prev and next members are meaningful.

   Every LWP in one of the lwp_state_*_interesting states should be on
   this queue.  If an LWP's state is lwp_state_dead_interesting, the
   LWP is not in the hash table any more.  */
static struct lwp interesting_queue
= { -1, 0, 0, &interesting_queue, &interesting_queue, 42 };


static const char *
wait_status_str (int status)
{
  static char buf[100];

  if (WIFSTOPPED (status))
    sprintf (buf, "WIFSTOPPED (s) && WSTOPSIG (s) == %d (%s)",
	     WSTOPSIG (status), strsignal (WSTOPSIG (status)));
  else if (WIFEXITED (status))
    sprintf (buf, "WIFEXITED (s) && WEXITSTATUS (s) == %d",
	     WEXITSTATUS (status));
  else if (WIFSIGNALED (status))
    sprintf (buf, "WIFSIGNALED (s) && WTERMSIG (s) == %d (%s)%s",
	     WTERMSIG (status),
	     strsignal (WTERMSIG (status)),
	     WCOREDUMP (status) ? " && WCOREDUMP(s)" : "");
  else
    sprintf (buf, "%d (unrecognized status)", status);

  return buf;
}


static const char *
wait_flags_str (int flags)
{
  static const struct {
    int flag;
    const char *name;
  } flag_table[] = {
    { WNOHANG, "WNOHANG" },
    { WUNTRACED, "WUNTRACED" },
#ifdef __WCLONE
    { __WCLONE, "__WCLONE" },
#endif
#ifdef __WALL
    { __WALL, "__WALL" },
#endif
#ifdef __WNOTHREAD
    { __WNOTHREAD, "__WNOTHREAD" },
#endif
    { 0, 0 }
  };
  static char buf[100];
  int i;

  buf[0] = '\0';
  for (i = 0; flag_table[i].flag; i++)
    if (flags & flag_table[i].flag)
      {
	strcat (buf, flag_table[i].name);
	flags &= ~flag_table[i].flag;
	if (flags)
	  strcat (buf, " | ");
      }

  if (flags)
    sprintf (buf + strlen (buf), "0x%x", (unsigned) flags);

  return buf;
}


static const char *
lwp_state_str (enum lwp_state state)
{
  switch (state)
    {
    case lwp_state_uninitialized:
      return "uninitialized";
    case lwp_state_running:
      return "running";
    case lwp_state_stopped:
      return "stopped";
    case lwp_state_stopped_interesting:
      return "stopped_interesting";
    case lwp_state_dead_interesting:
      return "dead_interesting";
    case lwp_state_running_stop_pending:
      return "running_stop_pending";
    case lwp_state_stopped_stop_pending_interesting:
      return "stopped_stop_pending_interesting";
    default:
      {
	static char buf[100];
	sprintf (buf, "%d (unrecognized lwp_state)", state);
	return buf;
      }
    }
}


static void
debug_report_state_change (pid_t lwp,
			   enum lwp_state old,
			   enum lwp_state new)
{
  if (debug_lwp_pool && old != new)
    fprintf (stderr,
	     "%32s -- %5d -> %-32s\n",
	     lwp_state_str (old), (int) lwp, lwp_state_str (new));
}


/* Wait for a status from the LWP L (or any LWP, if L is NULL),
   passing FLAGS to waitpid, and record the resulting wait status in
   the LWP pool appropriately.

   If no wait status was available (if FLAGS & WNOHANG), return zero.
   If we successfully processed some wait status, return 1.  If an
   error occurs, set errno and return -1.

   If waitpid returns an error, print a message to stderr.  */
static int
wait_and_handle (struct lwp *l, int flags)
{
  int status;
  pid_t new_pid; 
  enum lwp_state old_state;
  
  /* We can only wait for LWP's that are running.  */
  if (l)
    assert (l->state == lwp_state_running
	    || l->state == lwp_state_running_stop_pending);

  /* This should be the only call to waitpid in this module, to ensure
     that we always keep each LWP's state up to date.  In fact, it
     should be the only call to waitpid used by any module using the
     LWP pool code at all.  */
  new_pid = waitpid (l ? l->pid : -1, &status, flags);

  if (debug_lwp_pool)
    {
      fprintf (stderr,
	       "lwp_pool: wait_and_handle: waitpid (%d, %s, %s) == %d\n",
	       l ? l->pid : -1,
	       (new_pid <= 0 ? "(unset)" : wait_status_str (status)),
	       wait_flags_str (flags),
	       new_pid);
    }

  if (new_pid == -1)
    {
      /* If we call fprintf, that'll wipe out the value of errno.  */
      int saved_errno = errno;

      fprintf (stderr, "ERROR: waitpid (%d) failed: %s\n",
	       l ? (int) l->pid : -1,
	       strerror (saved_errno));

      errno = saved_errno;
      return -1;
    }

  if (new_pid == 0)
    /* No status, so no LWP has changed state.  */
    return 0;

  if (l)
    {
      if (l->pid != new_pid)
	{
	  fprintf (stderr, "ERROR: waited for %d, but got %d\n",
		   l->pid, new_pid);
	  l = hash_find_known (new_pid);
	}
    }
  else
    l = hash_find_known (new_pid);

  old_state = l->state;
  
  l->status = status;

  if (WIFEXITED (status) || WIFSIGNALED (status))
    {
      /* Remove dead LWP's from the hash table, and put them in the
	 interesting queue.  */
      hash_delete (l);
      l->state = lwp_state_dead_interesting;
      if (l->next)
	queue_delete (l);
      queue_enqueue (&interesting_queue, l);
    }
  else
    {
      int stopsig;

      assert (WIFSTOPPED (status));
      
      stopsig = WSTOPSIG (status);

      switch (l->state)
	{
	case lwp_state_uninitialized:
	  /* Might as well clean it up.  */
	case lwp_state_running:
	  /* It stopped, but not because of anything we did, so it's
	     interesting even if it was a SIGSTOP.  */
	  l->state = lwp_state_stopped_interesting;
	  queue_enqueue (&interesting_queue, l);
	  break;

	case lwp_state_running_stop_pending:

	  /* If we were in stopping_queue, we're stopped now.  */
	  if (l->next)
	    queue_delete (l);

	  /* We are expecting a boring SIGSTOP.  Is this it?  */
	  if (stopsig == SIGSTOP)
	    l->state = lwp_state_stopped;
	  else
	    {
	      /* Report this status, but remember that we're still
		 expecting the boring SIGSTOP.  */
	      l->state = lwp_state_stopped_stop_pending_interesting;
	      queue_enqueue (&interesting_queue, l);
	    }
	  break;

	default:
	  /* The assert at top should prevent any other states from
	     showing up here.  */
	  fprintf (stderr, "ERROR: called waitpid on LWP %d in bad state %s\n",
		   (int) l->pid, lwp_state_str (l->state));
	  abort ();
	  break;
	}
    }

  debug_report_state_change (l->pid, old_state, l->state);

  return 1;
}


/* Wait for a pending stop on the running LWP L.  Return non-zero if L
   ends up in an interesting state, or zero if L ends up in
   lwp_state_stopped.

   Whenever we have an LWP with no interesting status, but with a stop
   pending, we can always wait on it:

   - Since SIGCONT can't be blocked, caught, or ignored, the wait will
     always return immediately.  The process won't run amok.

   - Since the LWP is uninteresting to begin with, we'll end up with
     at most one interesting wait status to report; no need to queue
     up multiple statuses per LWP (which we'd rather not implement if
     we can avoid it).

   By always waiting immediately, we avoid the need for a state like
   lwp_state_stopped_stop_pending.

   So, this function takes a thread in lwp_state_running_stop_pending,
   and puts that thread in either lwp_state_stopped (no stop pending)
   or some INTERESTING state.  It's really just
   wait_and_handle, with some error checking wrapped around
   it.  */
static int
check_stop_pending (struct lwp *l)
{
  assert (l->state == lwp_state_running_stop_pending);

  wait_and_handle (l, __WALL);

  switch (l->state)
    {
    case lwp_state_stopped:
      return 0;

    case lwp_state_stopped_stop_pending_interesting:
    case lwp_state_stopped_interesting:
    case lwp_state_dead_interesting:
      return 1;

    default:
      fprintf (stderr,
	       "ERROR: checking lwp %d for pending stop yielded "
	       "bad state %s\n",
	       (int) l->pid, lwp_state_str (l->state));
      abort ();
      break;
    }
}


pid_t
lwp_pool_waitpid (pid_t pid, int *stat_loc, int options)
{
  struct lwp *l;
  enum lwp_state old_state;
  
  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_waitpid (%d, stat_loc, %s)\n",
	     (int) pid, wait_flags_str (options));

  /* Check that we're not being passed arguments that would be
     meaningful for the real waitpid, but that we can't handle.  */
  assert (pid == -1 || pid > 0);
  assert (! (options & ~WNOHANG));

  /* Do the wait, and choose an LWP to report on.  */
  if (pid == -1)
    {
      /* Handle wait statuses of any sort until something appears on
	 the interesting queue.  */
      while (! queue_non_empty (&interesting_queue))
	{
	  int result = wait_and_handle (NULL, options | __WALL);

	  if (result <= 0)
	    return result;
	}

      l = queue_first (&interesting_queue);
    }
  else
    {
      /* Waiting for a status from a specific pid PID.  */
      l = hash_find_known (pid);

      /* We should only wait for known, running LWP's.  */
      assert (l->state == lwp_state_running
	      || l->state == lwp_state_running_stop_pending);

      /* Wait until this pid is no longer running.  */
      while (l->state == lwp_state_running
	     || l->state == lwp_state_running_stop_pending)
	{
	  int result = wait_and_handle (l, options | __WALL);

	  if (result <= 0)
	    return result;
	}
    }

  /* Gather info from L early, in case we free it.  */
  pid = l->pid;
  old_state = l->state;
  if (stat_loc)
    *stat_loc = l->status;

  /* The INTERESTING states specifically mean that the LWP has a
     status which should be reported to the user, but that hasn't been
     yet.  Now we're about to report that status, so we need to mark
     interesting LWP's as uninteresting.  */
  switch (l->state)
    {
    case lwp_state_uninitialized:
    case lwp_state_running:
    case lwp_state_stopped:
    case lwp_state_running_stop_pending:
      /* These are uninteresting states.  The waiting code above
	 should never have chosen an LWP in one of these states.  */
      fprintf (stderr,
	       "ERROR: %s: selected uninteresting LWP %d state %s\n",
	       __func__, l->pid, lwp_state_str (l->state));
      abort ();
      break;

    case lwp_state_stopped_interesting:
      /* Now that we've reported this wait status to the user, the LWP
	 is not interesting any more.  */
      l->state = lwp_state_stopped;
      queue_delete (l);
      debug_report_state_change (l->pid, old_state, l->state);
      break;

    case lwp_state_dead_interesting:
      /* Once we've reported this status, we have washed our hands of
	 this LWP entirely.  */
      queue_delete (l);
      free (l);
      if (debug_lwp_pool)
	fprintf (stderr, 
		 "lwp_pool: %s: LWP %d state dead_interesting -> freed\n",
		 __func__, pid);
      break;

    case lwp_state_stopped_stop_pending_interesting:
      /* We're about to report this LWP's status, making it
	 uninteresting, but it's still got a stop pending.  So a state
	 like lwp_state_stopped_stop_pending would seem reasonable.

	 However, this is the only place such a state would occur.  By
	 removing the LWP from the interesting queue and continuing
	 it, we can go directly from
	 lwp_state_stopped_stop_pending_interesting to
	 lwp_state_running_stop_pending.

	 Since SIGSTOP cannot be blocked, caught, or ignored, we know
	 continuing the LWP won't actually allow it to run anywhere;
	 it just allows it to report another status.  */
      queue_delete (l);
      continue_lwp (l->pid, 0);
      l->state = lwp_state_running_stop_pending;
      debug_report_state_change (l->pid, old_state, l->state);
      check_stop_pending (l);
      break;

    default:
      fprintf (stderr, "ERROR: lwp %d in bad state: %s\n",
	       (int) l->pid, lwp_state_str (l->state));
      abort ();
      break;
    }

  return pid;
}



/* libthread_db-based death handling, for NPTL.  */


static const char *
death_state_str (enum death_state d)
{
  switch (d)
    {
    case death_state_running: return "death_state_running";
    case death_state_event_received: return "death_state_event_received";
    case death_state_delete_when_continued: 
      return "death_state_delete_when_continued";
    default:
      {
	static char buf[100];
	sprintf (buf, "%d (unrecognized death_state)", d);
	return buf;
      }
    }
}


static void
debug_report_death_state_change (pid_t lwp,
				 enum death_state old,
				 enum death_state new)
{
  if (debug_lwp_pool && old != new)
    fprintf (stderr,
	     "%32s -- %5d -> %-32s\n",
	     death_state_str (old), (int) lwp, death_state_str (new));
}


void
lwp_pool_thread_db_death_event (pid_t pid)
{
  struct lwp *l = hash_find_known (pid);
  enum death_state old_state = l->death_state;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_thread_db_death_event (%d)\n",
	     (int) pid);

  if (l->state == lwp_state_uninitialized)
    {
      /* hash_find_known has already complained about this; we just
	 clean up.  */
      hash_delete (l);
      free (l);
      return;
    }

  if (l->death_state == death_state_running)
    l->death_state = death_state_event_received;

  debug_report_death_state_change (pid, old_state, l->death_state);
}


void
lwp_pool_thread_db_death_notified (pid_t pid)
{
  struct lwp *l = hash_find_known (pid);
  enum death_state old_state = l->death_state;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_thread_db_death_notified (%d)\n",
	     (int) pid);

  if (l->state == lwp_state_uninitialized)
    {
      /* hash_find_known has already complained about this; we just
	 clean up.  */
      hash_delete (l);
      free (l);
      return;
    }

  if (l->death_state == death_state_event_received)
    l->death_state = death_state_delete_when_continued;

  debug_report_death_state_change (pid, old_state, l->death_state);
}


/* Subroutine for the 'continue' functions.  If the LWP L should be
   forgotten once continued, delete it from the hash table, and free
   its storage; we'll get no further wait status from it to indicate
   that it's gone.  */
static void
check_for_exiting_nptl_lwp (struct lwp *l)
{
  if (l->state == lwp_state_running
      && l->death_state == death_state_delete_when_continued)
    {
      if (debug_lwp_pool)
	fprintf (stderr,
		 "lwp_pool: %s: NPTL LWP %d will disappear silently\n",
		 __func__, l->pid);
      assert (! l->next && ! l->prev);
      hash_delete (l);
      free (l);
    }
}




/* Stopping and continuing.  */


void
lwp_pool_stop_all (void)
{
  int i;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_stop_all ()\n");

  /* The head of the queue of running LWP's that we are stopping.
     Only the prev and next members are meaningful.  */
  struct lwp stopping_queue;

  stopping_queue.next = stopping_queue.prev = &stopping_queue;

  /* First, put every LWP that's not already STOPPED or DEAD in a STOP
     PENDING state, and put them all on stopping_queue.  */ 
  for (i = 0; i < hash_size; i++)
    {
      struct lwp *l = hash[i];

      if (l)
	{
	  enum lwp_state old_state = l->state;

	  switch (l->state)
	    {
	      /* There should never be 'uninitialized' entries left in
		 the table.  Whoever created them ought to have put them
		 in some meaningful state before returning.  */
	    case lwp_state_uninitialized:
	      assert (l->state != lwp_state_uninitialized);
	      break;

	    case lwp_state_running:
	      /* A 'no such process' error here indicates an NPTL thread
		 that has exited.  */
	      kill_lwp (l->pid, SIGSTOP);
	      l->state = lwp_state_running_stop_pending;
	      queue_enqueue (&stopping_queue, l);
	      break;

	    case lwp_state_stopped:
	    case lwp_state_stopped_interesting:
	    case lwp_state_dead_interesting:
	    case lwp_state_stopped_stop_pending_interesting:
	      /* Nothing needs to be done here.  */
	      break;

	    case lwp_state_running_stop_pending:
	      /* Threads should never be in this state between calls to
		 public lwp_pool functions.  */
	      assert (l->state != lwp_state_running_stop_pending);
	      break;

	    default:
	      fprintf (stderr, "ERROR: lwp %d in bad state: %s\n",
		       (int) l->pid, lwp_state_str (l->state));
	      abort ();
	      break;
	    }

	  debug_report_state_change (l->pid, old_state, l->state);
	}
    }

  /* Gather wait results until the stopping queue is empty.  */
  while (queue_non_empty (&stopping_queue))
    if (wait_and_handle (NULL, __WALL) < 0)
      {
	fprintf (stderr, "ERROR: lwp_pool_stop_all wait failed: %s",
		 strerror (errno));
	return;
      }

  /* Now all threads should be stopped or dead.  But let's check.  */
  for (i = 0; i < hash_size; i++)
    {
      struct lwp *l = hash[i];
      if (l)
	switch (l->state)
	  {
	  case lwp_state_uninitialized:
	    assert (l->state != lwp_state_uninitialized);
	    break;

	  case lwp_state_running:
	  case lwp_state_running_stop_pending:
	    fprintf (stderr,
		     "ERROR: lwp_pool_stop_all failed: LWP %d still running\n",
		     (int) l->pid);
	    break;

	  case lwp_state_stopped:
	  case lwp_state_stopped_interesting:
	  case lwp_state_dead_interesting:
	  case lwp_state_stopped_stop_pending_interesting:
	    /* That's all as it should be.  */
	    break;

	  default:
	    fprintf (stderr, "ERROR: lwp %d in bad state: %s\n",
		     (int) l->pid, lwp_state_str (l->state));
	    abort ();
	    break;
	  }
    }
}


void
lwp_pool_continue_all (void)
{
  int i;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_continue_all ()\n");

  /* This loop makes every LWP either INTERESTING, or RUNNING.  */
  for (i = 0; i < hash_size; i++)
    {
      struct lwp *l = hash[i];

      if (l)
	{
	  enum lwp_state old_state = l->state;

	  switch (l->state)
	    {
	      /* There should never be 'uninitialized' entries left in
		 the table.  Whoever created them ought to have put them
		 in some meaningful state before returning.  */
	    case lwp_state_uninitialized:
	      assert (l->state != lwp_state_uninitialized);
	      break;

	    case lwp_state_running:
	      /* It's already running, so nothing needs to be done.  */
	      break;

	    case lwp_state_stopped:
	      if (continue_lwp (l->pid, 0) == 0)
		l->state = lwp_state_running;
	      break;

	    case lwp_state_stopped_interesting:
	    case lwp_state_dead_interesting:
	    case lwp_state_stopped_stop_pending_interesting:
	      /* We still have an unreported wait status here, so leave it
		 alone; we'll report it.  */
	      break;

	    case lwp_state_running_stop_pending:
	      /* There shouldn't be any threads in this state at this
		 point.  We should be calling check_stop_pending or
		 wait_and_handle as soon as we create them.  */
	      assert (l->state != lwp_state_running_stop_pending);
	      break;

	    default:
	      fprintf (stderr, "ERROR: lwp %d in bad state: %s\n", 
		       (int) l->pid, lwp_state_str (l->state));
	      abort ();
	      break;
	    }

	  debug_report_state_change (l->pid, old_state, l->state);

	  check_for_exiting_nptl_lwp (l);
	}
    }
}


int
lwp_pool_continue_lwp (pid_t pid, int signal)
{
  struct lwp *l = hash_find_known (pid);
  enum lwp_state old_state = l->state;
  int result;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_continue_lwp (%d, %d)\n",
	     (int) pid, signal);

  /* We should only be continuing stopped threads, with no interesting
     status to report.  And we should have cleaned up any pending
     stops as soon as we created them.  */
  assert (l->state == lwp_state_stopped);
  result = continue_lwp (l->pid, signal);
  if (result == 0)
    l->state = lwp_state_running;
  debug_report_state_change (l->pid, old_state, l->state);

  check_for_exiting_nptl_lwp (l);

  return result;
}


int
lwp_pool_singlestep_lwp (struct gdbserv *serv, pid_t lwp, int signal)
{
  struct lwp *l = hash_find_known (lwp);
  enum lwp_state old_state = l->state;
  int result;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_singlestep_lwp (%p, %d, %d)\n",
	     serv, (int) lwp, signal);

  /* We should only be single-stepping known, stopped threads, with no
     interesting status to report.  And we should have cleaned up any
     pending stops as soon as we created them.  */
  assert (l->state == lwp_state_stopped);
  result = singlestep_lwp (serv, l->pid, signal);
  if (result == 0)
    l->state = lwp_state_running;
  debug_report_state_change (l->pid, old_state, l->state);
  return result;
}



/* Adding new LWP's to the pool.  */

void
lwp_pool_new_stopped (pid_t pid)
{
  struct lwp *l = hash_find_new (pid);

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_new_stopped (%d)\n", (int) pid);

  l->state = lwp_state_stopped;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool: %s: new LWP %d state %s\n",
	     __func__, l->pid, lwp_state_str (l->state));
}


int
lwp_pool_attach (pid_t pid)
{
  /* Are we already managing this LWP?  */
  struct lwp *l = hash_find (pid);

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_attach (%d)\n", (int) pid);

  if (l->state == lwp_state_uninitialized)
    {
      /* No, we really need to attach to it.  */
      int status = attach_lwp (pid);

      if (status)
	{
	  /* Forget about the lwp.  */
	  hash_delete (l);
	  free (l);
	  return status;
	}

      /* Since we attached to it, we'll get a SIGSTOP for this
	 eventually.  Wait for it now, to put it in either
	 lwp_state_stopped, or in some interesting state.  */
      l->state = lwp_state_running_stop_pending;

      if (debug_lwp_pool)
	fprintf (stderr, "lwp_pool: %s: new LWP %d state %s\n",
		 __func__, l->pid, lwp_state_str (l->state));

      check_stop_pending (l);

      return 1;
    }
     
  return 0;
}
@


1.1.2.2
log
@* lwp-pool.c (wait_flags_str): Return "0" when FLAGS == 0, not the
empty string.
@
text
@d723 1
a723 4
  if (buf[0] == '\0')
    return "0";
  else
    return buf;
@


1.1.2.3
log
@Move libthread_db event-based thread death logic from lwp-pool.c
to thread-db.c, so the former can stay innocent of libthread_db's
details, and limit itself to kernel behavior.
* lwp-pool.h (lwp_pool_continue_and_drop): New declaration.
(lwp_pool_thread_db_death_event,
lwp_pool_thread_db_death_notified): Declarations deleted.
* lwp-pool.c (enum death_state): Move to thread-db.c.
(struct lwp): Delete 'death_state' member.
(hash_find): Don't initialize it.
(interesting_queue): Don't provide an initializer for it.
(check_stop_pending): Don't abort if wait_and_handle didn't put
the thread in a stopped state; that could be caused by kernel
behavior (say, threads exiting silently), not necessarily a logic
flaw in lwp-pool.c.
(death_state_str, debug_report_death_state_change): Move to
thread-db.c.
(lwp_pool_thread_db_death_event,
lwp_pool_thread_db_death_notified): Move to thread-db.c, and
rename (see below).
(check_for_exiting_nptl_lwp): Delete.
(lwp_pool_continue_all, lwp_pool_continue_lwp): Don't call
check_for_exiting_nptl_lwp.
(lwp_pool_continue_and_drop_lwp): New function.
* thread-db.c (enum death_state): Moved here from lwp-pool.c.
(struct gdbserv_thread): New member death_state.
(add_thread_to_list): Initialize it.
(death_state_str, debug_report_death_state_change): Moved here
from lwp-pool.c.
(death_state_got_event, death_state_notified): Renamed from
lwp_pool_thread_db_death_event and lwp_pool_thread_db_death_notified,
and changed to update the death state of a 'struct gdbserv_thread'
instead of a 'struct lwp'.
(handle_thread_db_event): Call death_state_got_event instead of
lwp_pool_thread_db_death_event, and death_state_notified instead
of lwp_pool_thread_db_death_notified.
(continue_thread): If the thread's death state indicates that it's
going to disappear without further ado when continued, continue it
with lwp_pool_continue_and_drop_lwp, not lwp_pool_continue_lwp.
@
text
@d265 19
d292 3
d468 1
d661 1
a661 1
= { -1, 0, &interesting_queue, &interesting_queue, 42 };
d949 2
a950 5
      hash_delete (l);
      if (l->next)
	queue_delete (l);
      free (l);
      return 0;
d1081 109
d1359 2
d1386 1
a1386 29
  return result;
}


int
lwp_pool_continue_and_drop_lwp (pid_t pid, int signal)
{
  struct lwp *l = hash_find_known (pid);
  int result;

  if (debug_lwp_pool)
    fprintf (stderr, "lwp_pool_continue_and_drop_lwp (%d, %d)\n",
	     (int) pid, signal);

  /* We should only be continuing stopped threads, with no interesting
     status to report.  And we should have cleaned up any pending
     stops as soon as we created them.  */
  assert (l->state == lwp_state_stopped);
  result = continue_lwp (l->pid, signal);
  if (result == 0)
    {
      hash_delete (l);
      assert (! l->next && ! l->prev);
      free (l);

      if (debug_lwp_pool)
	fprintf (stderr, "                         stopped -- %d --> freed\n",
		 (int) pid);
    }
@


1.1.2.4
log
@Introduce a new STOPPED, STOP PENDING state to the LWP pool model.
* lwp-pool.c: Many doc fixes.
(enum lwp_state): Add new value, lwp_state_stopped_stop_pending.
(lwp_state_str): Provide a name for it.
(lwp_pool_waitpid): Reject it as an initial state for an LWP whose
status we'll report.  Produce it when we report the status of a
STOPPED, STOP PENDING, INTERESTING LWP, rather than continuing
the LWP and waiting for it again.
(lwp_pool_stop_all): Recognize it as a stopped state.
(lwp_pool_continue_all, lwp_pool_continue_lwp,
lwp_pool_singlestep_lwp): Continue and check for the boring status
here, instead of in lwp_pool_waitpid.
(lwp_pool_continue_and_drop_lwp): Just call lwp_pool_continue_lwp,
instead of writing out its contents again here.
@
text
@d72 2
a73 1
     pending on the LWP, and will be reported by waitpid.
d81 2
a82 5
     The ptrace requests all let you specify a signal to be delivered
     to the process.  This is the only way signals (other than
     SIGKILL) ever get actually delivered: every other signal just
     gets reported to the debugger via waitpid when delivery is
     attempted.
d115 2
a116 2
   - Running->stopped transitions are always attempted signal
     deliveries, yielding WIFSTOPPED wait statuses.
d119 2
a120 2
     requests by the debugger.  (The debugger could use kill to send
     SIGCONT, but that's messy.)
d204 9
a256 8
  /* STOPPED, STOP PENDING.  This LWP is stopped, and has no
     interesting status to report, but still has a boring status on
     the way.  After we report the status for a STOPPED, STOP PENDING,
     and INTERESTING thread, this is the state it enters.

     See the note below on why this state is not avoidable.  */
  lwp_state_stopped_stop_pending,

a264 35
/* Why we need lwp_state_stopped_stop_pending:

   I originally thought we could avoid having this state at all by
   simply always continuing STOPPED, STOP PENDING, INTERESTING threads
   in lwp_pool_waitpid as soon as we reported their wait status, and
   then waiting for them immediately, making them either STOPPED and
   un-INTERESTING, or STOPPED, STOP PENDING, and INTERESTING again.

   But the user has the right to call lwp_pool_continue_lwp on any LWP
   they've just gotten a wait status for --- and this simplification
   interferes with that.  First, note that you mustn't call
   continue_lwp on an interesting LWP: you might get yet another
   interesting wait status, and we don't want to queue up multiple
   interesting wait statuses per LWP --- the job is complex enough
   already.  Then, note that the proposed simplification means that
   lwp_pool_waitpid could return a status for some LWP, and have that
   LWP still be interesting.  If that happens, then you've got an LWP
   the user has the right to continue, but that can't actually be
   continued.

   I first tried to deal with this by having lwp_pool_continue_lwp
   simply do nothing if the user continues an interesting LWP.  After
   all, it's already in the interesting queue, so lwp_pool_waitpid
   will report it, and the user will be none the wiser.  But that's
   wrong: the user can specify a signal to deliver when they continue
   the LWP, and the only way signals are ever delivered to traced LWPs
   is via ptrace continue and single-step requests.  You can't use
   kill: that *generates* a signal, it doesn't *deliver* it.  You'd
   just get the signal back again via waitpid.  So if we don't
   actually continue the LWP with the user's signal, we've lost our
   only chance to deliver it.

   Clear as mud, no doubt.  I did my best.  */


a723 2
    case lwp_state_stopped_stop_pending:
      return "stopped_stop_pending";
d896 3
a999 1
    case lwp_state_stopped_stop_pending:
d1030 12
a1041 1
	 uninteresting, but it's still got a stop pending.  */
d1043 2
a1044 1
      l->state = lwp_state_stopped_stop_pending;
d1046 1
a1105 1
            case lwp_state_stopped_stop_pending:
a1156 1
          case lwp_state_stopped_stop_pending:
a1221 16
            case lwp_state_stopped_stop_pending:
              /* Continue it, and then wait for the pending stop.
                 Since SIGSTOP cannot be blocked, caught, or ignored,
                 the wait will always return immediately; the LWP
                 won't run amok.  */
              if (continue_lwp (l->pid, 0) == 0)
                {
                  l->state = lwp_state_running_stop_pending;
                  if (check_stop_pending (l) == 0)
                    {
                      if (continue_lwp (l->pid, 0) == 0)
                        l->state = lwp_state_running;
                    }
                }
              break;

d1240 1
a1240 1
  int result = 0;
d1246 7
a1252 52
  switch (l->state)
    {
    case lwp_state_uninitialized:
      assert (l->state != lwp_state_uninitialized);
      break;

      /* We should only be continuing LWPs that have reported a
         WIFSTOPPED status via lwp_pool_waitpid and have not been
         continued or singlestepped since.  */
    case lwp_state_running:
    case lwp_state_stopped_interesting:
    case lwp_state_dead_interesting:
    case lwp_state_running_stop_pending:
    case lwp_state_stopped_stop_pending_interesting:
      fprintf (stderr, "ERROR: continuing LWP %d in unwaited state: %s\n",
               (int) l->pid, lwp_state_str (l->state));
      break;

    case lwp_state_stopped:
      result = continue_lwp (l->pid, signal);
      if (result == 0)
        l->state = lwp_state_running;
      break;

    case lwp_state_stopped_stop_pending:
      /* Continue it, delivering the given signal, and then wait for
         the pending stop.  Since SIGSTOP cannot be blocked, caught,
         or ignored, the wait will always return immediately; the LWP
         won't run amok.

         We must deliver the signal with the first continue_lwp call;
         if check_stop_pending says the LWP has a new interesting
         status, then we'll never reach the second continue_lwp, and
         we'll lose our chance to deliver the signal.  */
      if (continue_lwp (l->pid, signal) == 0)
        {
          l->state = lwp_state_running_stop_pending;
          if (check_stop_pending (l) == 0)
            {
              if (continue_lwp (l->pid, 0) == 0)
                l->state = lwp_state_running;
            }
        }
      break;

    default:
      fprintf (stderr, "ERROR: lwp %d in bad state: %s\n", 
               (int) l->pid, lwp_state_str (l->state));
      abort ();
      break;
    }

d1263 1
a1263 1
  int result = 0;
d1269 5
a1273 1
  result = lwp_pool_continue_lwp (l->pid, signal);
d1277 1
a1277 2
      if (l->next)
        queue_delete (l);
d1281 2
a1282 3
        fprintf (stderr,
                 "                         stopped -- %d --> freed\n",
                 (int) pid);
d1287 1
a1287 1
  
d1294 1
a1294 1
  int result = 0;
d1300 7
a1306 52
  switch (l->state)
    {
    case lwp_state_uninitialized:
      assert (l->state != lwp_state_uninitialized);
      break;

      /* We should only be stepping LWPs that have reported a
         WIFSTOPPED status via lwp_pool_waitpid and have not been
         continued or singlestepped since.  */
    case lwp_state_running:
    case lwp_state_stopped_interesting:
    case lwp_state_dead_interesting:
    case lwp_state_running_stop_pending:
    case lwp_state_stopped_stop_pending_interesting:
      fprintf (stderr, "ERROR: stepping LWP %d in unwaited state: %s\n",
               (int) l->pid, lwp_state_str (l->state));
      break;

    case lwp_state_stopped:
      result = singlestep_lwp (serv, l->pid, signal);
      if (result == 0)
        l->state = lwp_state_running;
      break;

    case lwp_state_stopped_stop_pending:
      /* Continue it, delivering the given signal, and then wait for
         the pending stop.  Since SIGSTOP cannot be blocked, caught,
         or ignored, the wait will always return immediately; the LWP
         won't run amok.

         We must deliver the signal with the continue_lwp call; if
         check_stop_pending says the LWP has a new interesting status,
         then we'll never reach the singlestep_lwp, and we'll lose our
         chance to deliver the signal at all.  */
      if (continue_lwp (l->pid, signal) == 0)
        {
          l->state = lwp_state_running_stop_pending;
          if (check_stop_pending (l) == 0)
            {
              if (singlestep_lwp (serv, l->pid, 0) == 0)
                l->state = lwp_state_running;
            }
        }
      break;

    default:
      fprintf (stderr, "ERROR: lwp %d in bad state: %s\n", 
               (int) l->pid, lwp_state_str (l->state));
      abort ();
      break;
    }

a1307 1

@


1.1.2.5
log
@* lwp-pool.c (check_stop_pending): lwp_state_stopped_interesting
is not a legitimate state for the LWP to be in.
@
text
@d952 1
a955 5
    case lwp_state_stopped_interesting:
      /* This state shouldn't happen: since there was a pending stop,
         a single waitpid on that LWP should have either gotten the
         SIGSTOP, yielding 'lwp_state_stopped', or something interesting,
         yielding 'lwp_state_stopped_stop_pending_interesting'.  */
@


1.1.2.6
log
@* lwp-pool.c: Doc fixes.  (Use "LWP" instead of "thread".)
@
text
@d244 3
a246 3
     It's always safe to wait for an LWP in this state, so we do that
     as soon as possible; there shouldn't be any LWPs in this state
     between calls to public lwp_pool functions.  This is an
d253 1
a253 1
     and INTERESTING LWP, this is the state it enters.
d269 1
a269 1
   simply always continuing STOPPED, STOP PENDING, INTERESTING LWPs
d314 2
a315 2
     stopping_queue is local to lwp_pool_stop_all; no LWP should be in
     that state by the time that function returns.  */
d934 5
a938 4
   So, this function takes an LWP in lwp_state_running_stop_pending,
   and puts that LWP in either lwp_state_stopped (no stop pending) or
   some INTERESTING state.  It's really just wait_and_handle, with
   some error checking wrapped around it.  */
d1141 1
a1141 1
	      /* LWPs should never be in this state between calls to
d1166 1
a1166 1
  /* Now all LWPs should be stopped or dead.  But let's check.  */
d1245 1
a1245 1
	      /* There shouldn't be any LWPs in this state at this
@


1.1.2.7
log
@Remove code to handle threads exiting without generating a wait
status; that was a kernel bug, not an intended feature, and
there's no easy way to support both.
* lwp-pool.c (lwp_pool_continue_and_drop): Delete function.
* lwp-pool.h (lwp_pool_continue_and_drop): Delete declaration.
* thread-db.c (enum death_state): Delete type.
(struct gdbserv_thread): Remove 'death_state' member.
(add_thread_to_list): Don't initialize it.
(death_state_str, debug_report_death_state_change,
death_state_got_event, death_state_notified): Delete functions.
(handle_thread_db_event): Don't handle death events specially, and
don't handle the notifying thread specially.
(continue_thread): Don't treat threads whose death has been
foretold and who have completed their notification specially.
@
text
@d1349 28
@


1.1.2.8
log
@Disable debugging messages by default.
* lwp-pool.c (debug_lwp_pool): Initialize to zero.
* thread-db.c (thread_db_noisy): Same.
@
text
@d40 1
a40 1
static int debug_lwp_pool = 0;
@


