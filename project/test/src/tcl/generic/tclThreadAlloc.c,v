head	1.3;
access;
symbols
	sid-snapshot-20180601:1.3
	sid-snapshot-20180501:1.3
	sid-snapshot-20180401:1.3
	sid-snapshot-20180301:1.3
	sid-snapshot-20180201:1.3
	sid-snapshot-20180101:1.3
	sid-snapshot-20171201:1.3
	sid-snapshot-20171101:1.3
	sid-snapshot-20171001:1.3
	sid-snapshot-20170901:1.3
	sid-snapshot-20170801:1.3
	sid-snapshot-20170701:1.3
	sid-snapshot-20170601:1.3
	sid-snapshot-20170501:1.3
	sid-snapshot-20170401:1.3
	sid-snapshot-20170301:1.3
	sid-snapshot-20170201:1.3
	sid-snapshot-20170101:1.3
	sid-snapshot-20161201:1.3
	sid-snapshot-20161101:1.3
	sid-snapshot-20160901:1.3
	sid-snapshot-20160801:1.3
	sid-snapshot-20160701:1.3
	sid-snapshot-20160601:1.3
	sid-snapshot-20160501:1.3
	sid-snapshot-20160401:1.3
	sid-snapshot-20160301:1.3
	sid-snapshot-20160201:1.3
	sid-snapshot-20160101:1.3
	sid-snapshot-20151201:1.3
	sid-snapshot-20151101:1.3
	sid-snapshot-20151001:1.3
	sid-snapshot-20150901:1.3
	sid-snapshot-20150801:1.3
	sid-snapshot-20150701:1.3
	sid-snapshot-20150601:1.3
	sid-snapshot-20150501:1.3
	sid-snapshot-20150401:1.3
	sid-snapshot-20150301:1.3
	sid-snapshot-20150201:1.3
	sid-snapshot-20150101:1.3
	sid-snapshot-20141201:1.3
	sid-snapshot-20141101:1.3
	sid-snapshot-20141001:1.3
	sid-snapshot-20140901:1.3
	sid-snapshot-20140801:1.3
	sid-snapshot-20140701:1.3
	sid-snapshot-20140601:1.3
	sid-snapshot-20140501:1.3
	sid-snapshot-20140401:1.3
	sid-snapshot-20140301:1.3
	sid-snapshot-20140201:1.3
	sid-snapshot-20140101:1.3
	sid-snapshot-20131201:1.3
	sid-snapshot-20131101:1.3
	sid-snapshot-20131001:1.3
	sid-snapshot-20130901:1.3
	sid-snapshot-20130801:1.3
	sid-snapshot-20130701:1.3
	sid-snapshot-20130601:1.3
	insight_7_6-2013-04-10-branchpoint:1.3
	gdb_7_6-branch:1.3.0.60
	sid-snapshot-20130501:1.3
	sid-snapshot-20130401:1.3
	sid-snapshot-20130301:1.3
	sid-snapshot-20130201:1.3
	sid-snapshot-20130101:1.3
	sid-snapshot-20121201:1.3
	sid-snapshot-20121101:1.3
	sid-snapshot-20121001:1.3
	sid-snapshot-20120901:1.3
	gdb_7_5-branch:1.3.0.58
	sid-snapshot-20120801:1.3
	sid-snapshot-20120701:1.3
	sid-snapshot-20120601:1.3
	sid-snapshot-20120501:1.3
	sid-snapshot-20120401:1.3
	gdb_7_4-branch:1.3.0.56
	sid-snapshot-20120301:1.3
	sid-snapshot-20120201:1.3
	sid-snapshot-20120101:1.3
	sid-snapshot-20111201:1.3
	sid-snapshot-20111101:1.3
	sid-snapshot-20111001:1.3
	sid-snapshot-20110901:1.3
	gdb_7_3-branch:1.3.0.54
	sid-snapshot-20110801:1.3
	sid-snapshot-20110701:1.3
	sid-snapshot-20110601:1.3
	sid-snapshot-20110501:1.3
	sid-snapshot-20110401:1.3
	sid-snapshot-20110301:1.3
	sid-snapshot-20110201:1.3
	sid-snapshot-20110101:1.3
	sid-snapshot-20101201:1.3
	sid-snapshot-20101101:1.3
	sid-snapshot-20101001:1.3
	sid-snapshot-20100901:1.3
	sid-snapshot-20100801:1.3
	sid-snapshot-20100701:1.3
	sid-snapshot-20100601:1.3
	sid-snapshot-20100501:1.3
	sid-snapshot-20100401:1.3
	sid-snapshot-20100301:1.3
	gdb_7_1-branch:1.3.0.52
	gdb_7_0-branch:1.3.0.50
	sid-snapshot-20100201:1.3
	sid-snapshot-20100101:1.3
	sid-snapshot-20091201:1.3
	sid-snapshot-20091101:1.3
	sid-snapshot-20091001:1.3
	arc-sim-20090309:1.3
	sid-snapshot-20090901:1.3
	sid-snapshot-20090801:1.3
	sid-snapshot-20090701:1.3
	sid-snapshot-20090601:1.3
	sid-snapshot-20090501:1.3
	kevinb-pre-tcl8_5_7_merge:1.3
	sid-snapshot-20090401:1.3
	arc-insight_6_8-branch:1.3.0.48
	arc-insight_6_8-branchpoint:1.3
	insight_6_8-branch:1.3.0.46
	insight_6_8-branchpoint:1.3
	sid-snapshot-20090301:1.3
	sid-snapshot-20090201:1.3
	sid-snapshot-20090101:1.3
	sid-snapshot-20081201:1.3
	sid-snapshot-20081101:1.3
	sid-snapshot-20081001:1.3
	sid-snapshot-20080901:1.3
	sid-snapshot-20080801:1.3
	sid-snapshot-20080701:1.3
	sid-snapshot-20080601:1.3
	sid-snapshot-20080501:1.3
	sid-snapshot-20080403:1.3
	sid-snapshot-20080401:1.3
	gdb_6_8-branch:1.3.0.44
	sid-snapshot-20080301:1.3
	sid-snapshot-20080201:1.3
	sid-snapshot-20080101:1.3
	sid-snapshot-20071201:1.3
	sid-snapshot-20071101:1.3
	sid-snapshot-20071001:1.3
	insight_6_6-20070208-release:1.3
	gdb_6_6-branch:1.3.0.42
	gdb_6_6-2006-11-15-branchpoint:1.3
	insight_6_5-20061003-release:1.3
	gdb_6_5-branch:1.3.0.40
	gdb_6_5-2006-05-14-branchpoint:1.3
	readline_5_1-import-branch:1.3.0.38
	readline_5_1-import-branchpoint:1.3
	gdb_6_4-branch:1.3.0.36
	gdb_6_4-2005-11-01-branchpoint:1.3
	msnyder-tracepoint-checkpoint-branch:1.3.0.34
	msnyder-tracepoint-checkpoint-branchpoint:1.3
	gdb_6_1-2004-04-05-release:1.3
	ezannoni_pie-20040323-branch:1.3.0.32
	ezannoni_pie-20040323-branchpoint:1.3
	cagney_tramp-20040321-mergepoint:1.3
	cagney_tramp-20040309-branch:1.3.0.30
	cagney_tramp-20040309-branchpoint:1.3
	gdb_6_1-branch:1.3.0.28
	gdb_6_1-2004-03-01-gmt-branchpoint:1.3
	drow-cplus-merge-20040208:1.3
	carlton_dictionary-20040126-merge:1.3
	drow-cplus-merge-20040113:1.3
	drow-cplus-merge-20031224:1.3
	drow-cplus-merge-20031220:1.3
	carlton_dictionary-20031215-merge:1.3
	drow-cplus-merge-20031214:1.3
	carlton-dictionary-20031111-merge:1.3
	gdb_6_0-2003-10-04-release:1.3
	carlton_dictionary-20030917-merge:1.3
	ezannoni_pie-20030916-branchpoint:1.3
	ezannoni_pie-20030916-branch:1.3.0.26
	gdb_5_1-2001-07-29-branch:1.3.0.24
	cagney_x86i386-20030821-branch:1.3.0.22
	cagney_x86i386-20030821-branchpoint:1.3
	carlton_dictionary-20030805-merge:1.3
	carlton_dictionary-20030627-merge:1.3
	gdb_6_0-branch:1.3.0.20
	gdb_6_0-2003-06-23-branchpoint:1.3
	cagney_convert-20030606-branch:1.3.0.18
	cagney_convert-20030606-branchpoint:1.3
	cagney_writestrings-20030508-branch:1.3.0.16
	cagney_writestrings-20030508-branchpoint:1.3
	carlton_dictionary-20030523-merge:1.3
	cagney_fileio-20030521-branch:1.3.0.14
	cagney_fileio-20030521-branchpoint:1.3
	carlton_dictionary-20030430-merge:1.3
	carlton_dictionary-20030416-merge:1.3
	cagney_frameaddr-20030409-mergepoint:1.3
	cagney_frameaddr-20030403-branchpoint:1.3
	cagney_frameaddr-20030403-branch:1.3.0.12
	cagney_framebase-20030330-mergepoint:1.3
	cagney_framebase-20030326-branch:1.3.0.10
	cagney_framebase-20030326-branchpoint:1.3
	cagney_lazyid-20030317-branch:1.3.0.8
	cagney_lazyid-20030317-branchpoint:1.3
	offbyone-20030313-branch:1.3.0.6
	offbyone-20030313-branchpoint:1.3
	carlton_dictionary-20030305-merge:1.3
	cagney_offbyone-20030303-branch:1.3.0.4
	cagney_offbyone-20030303-branchpoint:1.3
	carlton_dictionary-20030207-merge:1.3
	interps-20030202-branch:1.3.0.2
	interps-20030202-branchpoint:1.3
	TCL8_4_1:1.1.1.1
	cagney-unwind-20030108-branch:1.2.0.2
	cagney-unwind-20030108-branchpoint:1.2
	carlton_dictionary-20021223-merge:1.2
	TCL_8_4_1:1.1.1.1
	carlton_dictionary-20021115-merge:1.1
	kseitz_interps-20021105-merge:1.1
	kseitz_interps-20021103-merge:1.1
	drow-cplus-merge-20021020:1.1
	drow-cplus-merge-20021025:1.1
	carlton_dictionary-20021025-merge:1.1
	carlton_dictionary-20021011-merge:1.1
	drow-cplus-branch:1.1.0.6
	drow-cplus-branchpoint:1.1
	kseitz_interps-20020528-branch:1.1.0.4
	kseitz_interps-20020930-merge:1.1
	carlton_dictionary-branch:1.1.0.2
	carlton_dictionary-20020927-merge:1.1.1.1
	tcltk840-20020924-branch:1.1.1.1.0.2
	tcltk840-20020924-branchpoint:1.1.1.1
	TCL_8_4_0:1.1.1.1
	NET:1.1.1;
locks; strict;
comment	@ * @;


1.3
date	2003.01.21.19.40.06;	author hunt;	state Exp;
branches
	1.3.48.1;
next	1.2;

1.2
date	2002.11.26.19.47.53;	author hunt;	state Exp;
branches;
next	1.1;

1.1
date	2002.09.24.19.56.08;	author kseitz;	state Exp;
branches
	1.1.1.1
	1.1.2.1
	1.1.4.1
	1.1.6.1;
next	;

1.3.48.1
date	2009.09.11.04.45.59;	author amylaar;	state Exp;
branches;
next	;

1.1.1.1
date	2002.09.24.19.56.08;	author kseitz;	state Exp;
branches
	1.1.1.1.2.1;
next	;

1.1.1.1.2.1
date	2002.09.27.21.49.05;	author kseitz;	state Exp;
branches;
next	;

1.1.2.1
date	2002.09.27.20.03.34;	author carlton;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2002.10.11.22.23.10;	author carlton;	state Exp;
branches;
next	1.1.2.3;

1.1.2.3
date	2002.12.23.19.40.25;	author carlton;	state Exp;
branches;
next	1.1.2.4;

1.1.2.4
date	2003.02.07.19.18.14;	author carlton;	state Exp;
branches;
next	;

1.1.4.1
date	2002.10.01.00.46.55;	author kseitz;	state Exp;
branches;
next	;

1.1.6.1
date	2003.12.14.20.28.44;	author drow;	state Exp;
branches;
next	;


desc
@@


1.3
log
@Updated to tcl 8.4.1
@
text
@/*
 * tclThreadAlloc.c --
 *
 *	This is a very fast storage allocator for used with threads (designed
 *	avoid lock contention).  The basic strategy is to allocate memory in
 *  	fixed size blocks from block caches.
 * 
 * The Initial Developer of the Original Code is America Online, Inc.
 * Portions created by AOL are Copyright (C) 1999 America Online, Inc.
 *
 * See the file "license.terms" for information on usage and redistribution
 * of this file, and for a DISCLAIMER OF ALL WARRANTIES.
 *
 * RCS: @@(#) $Id: tclThreadAlloc.c,v 1.4 2002/08/26 13:05:56 msofer Exp $ */

#if defined(TCL_THREADS) && defined(USE_THREAD_ALLOC)

#include "tclInt.h"

#ifdef WIN32
#include "tclWinInt.h"
#else
extern Tcl_Mutex *TclpNewAllocMutex(void);
extern void *TclpGetAllocCache(void);
extern void TclpSetAllocCache(void *);
#endif

/*
 * If range checking is enabled, an additional byte will be allocated
 * to store the magic number at the end of the requested memory.
 */

#ifndef RCHECK
#ifdef  NDEBUG
#define RCHECK		0
#else
#define RCHECK		1
#endif
#endif

/*
 * The following define the number of Tcl_Obj's to allocate/move
 * at a time and the high water mark to prune a per-thread cache.
 * On a 32 bit system, sizeof(Tcl_Obj) = 24 so 800 * 24 = ~16k.
 *
 */
 
#define NOBJALLOC	 800
#define NOBJHIGH	1200

/*
 * The following defines the number of buckets in the bucket
 * cache and those block sizes from (1<<4) to (1<<(3+NBUCKETS))
 */

#define NBUCKETS	  11
#define MAXALLOC	  16284

/*
 * The following union stores accounting information for
 * each block including two small magic numbers and
 * a bucket number when in use or a next pointer when
 * free.  The original requested size (not including
 * the Block overhead) is also maintained.
 */
 
typedef struct Block {
    union {
    	struct Block *next;	  /* Next in free list. */
    	struct {
	    unsigned char magic1; /* First magic number. */
	    unsigned char bucket; /* Bucket block allocated from. */
	    unsigned char unused; /* Padding. */
	    unsigned char magic2; /* Second magic number. */
        } b_s;
    } b_u;
    size_t b_reqsize;		  /* Requested allocation size. */
} Block;
#define b_next		b_u.next
#define b_bucket	b_u.b_s.bucket
#define b_magic1	b_u.b_s.magic1
#define b_magic2	b_u.b_s.magic2
#define MAGIC		0xef

/*
 * The following structure defines a bucket of blocks with
 * various accounting and statistics information.
 */

typedef struct Bucket {
    Block *firstPtr;
    int nfree;
    int nget;
    int nput;
    int nwait;
    int nlock;
    int nrequest;
} Bucket;

/*
 * The following structure defines a cache of buckets and objs.
 */

typedef struct Cache {
    struct Cache  *nextPtr;
    Tcl_ThreadId   owner;
    Tcl_Obj       *firstObjPtr;
    int            nobjs;
    int	           nsysalloc;
    Bucket         buckets[NBUCKETS];
} Cache;

/*
 * The following array specifies various per-bucket 
 * limits and locks.  The values are statically initialized
 * to avoid calculating them repeatedly.
 */

struct binfo {
    size_t blocksize;	/* Bucket blocksize. */
    int maxblocks;	/* Max blocks before move to share. */
    int nmove;		/* Num blocks to move to share. */
    Tcl_Mutex *lockPtr; /* Share bucket lock. */
} binfo[NBUCKETS] = {
    {   16, 1024, 512, NULL},
    {   32,  512, 256, NULL},
    {   64,  256, 128, NULL},
    {  128,  128,  64, NULL},
    {  256,   64,  32, NULL},
    {  512,   32,  16, NULL},
    { 1024,   16,   8, NULL},
    { 2048,    8,   4, NULL},
    { 4096,    4,   2, NULL},
    { 8192,    2,   1, NULL},
    {16284,    1,   1, NULL},
};

/*
 * Static functions defined in this file.
 */

static void LockBucket(Cache *cachePtr, int bucket);
static void UnlockBucket(Cache *cachePtr, int bucket);
static void PutBlocks(Cache *cachePtr, int bucket, int nmove);
static int  GetBlocks(Cache *cachePtr, int bucket);
static Block *Ptr2Block(char *ptr);
static char *Block2Ptr(Block *blockPtr, int bucket, unsigned int reqsize);
static void MoveObjs(Cache *fromPtr, Cache *toPtr, int nmove);

/*
 * Local variables defined in this file and initialized at
 * startup.
 */

static Tcl_Mutex *listLockPtr;
static Tcl_Mutex *objLockPtr;
static Cache     sharedCache;
static Cache    *sharedPtr = &sharedCache;
static Cache    *firstCachePtr = &sharedCache;


/*
 *----------------------------------------------------------------------
 *
 *  GetCache ---
 *
 *	Gets per-thread memory cache, allocating it if necessary.
 *
 * Results:
 *	Pointer to cache.
 *
 * Side effects:
 *  	None.
 *
 *----------------------------------------------------------------------
 */

static Cache *
GetCache(void)
{
    Cache *cachePtr;

    /*
     * Check for first-time initialization.
     */

    if (listLockPtr == NULL) {
	Tcl_Mutex *initLockPtr;
    	int i;

	initLockPtr = Tcl_GetAllocMutex();
	Tcl_MutexLock(initLockPtr);
	if (listLockPtr == NULL) {
	    listLockPtr = TclpNewAllocMutex();
	    objLockPtr = TclpNewAllocMutex();
	    for (i = 0; i < NBUCKETS; ++i) {
	        binfo[i].lockPtr = TclpNewAllocMutex();
	    }
	}
	Tcl_MutexUnlock(initLockPtr);
    }

    /*
     * Get this thread's cache, allocating if necessary.
     */

    cachePtr = TclpGetAllocCache();
    if (cachePtr == NULL) {
    	cachePtr = calloc(1, sizeof(Cache));
    	if (cachePtr == NULL) {
	    panic("alloc: could not allocate new cache");
    	}
    	Tcl_MutexLock(listLockPtr);
    	cachePtr->nextPtr = firstCachePtr;
    	firstCachePtr = cachePtr;
    	Tcl_MutexUnlock(listLockPtr);
    	cachePtr->owner = Tcl_GetCurrentThread();
	TclpSetAllocCache(cachePtr);
    }
    return cachePtr;
}


/*
 *----------------------------------------------------------------------
 *
 *  TclFreeAllocCache --
 *
 *	Flush and delete a cache, removing from list of caches.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */

void
TclFreeAllocCache(void *arg)
{
    Cache *cachePtr = arg;
    Cache **nextPtrPtr;
    register int   bucket;

    /*
     * Flush blocks.
     */

    for (bucket = 0; bucket < NBUCKETS; ++bucket) {
	if (cachePtr->buckets[bucket].nfree > 0) {
	    PutBlocks(cachePtr, bucket, cachePtr->buckets[bucket].nfree);
	}
    }

    /*
     * Flush objs.
     */

    if (cachePtr->nobjs > 0) {
    	Tcl_MutexLock(objLockPtr);
    	MoveObjs(cachePtr, sharedPtr, cachePtr->nobjs);
    	Tcl_MutexUnlock(objLockPtr);
    }

    /*
     * Remove from pool list.
     */

    Tcl_MutexLock(listLockPtr);
    nextPtrPtr = &firstCachePtr;
    while (*nextPtrPtr != cachePtr) {
	nextPtrPtr = &(*nextPtrPtr)->nextPtr;
    }
    *nextPtrPtr = cachePtr->nextPtr;
    cachePtr->nextPtr = NULL;
    Tcl_MutexUnlock(listLockPtr);
#ifdef WIN32
    TlsFree((DWORD) cachePtr);
#else
    free(cachePtr);
#endif
}


/*
 *----------------------------------------------------------------------
 *
 *  TclpAlloc --
 *
 *	Allocate memory.
 *
 * Results:
 *	Pointer to memory just beyond Block pointer.
 *
 * Side effects:
 *	May allocate more blocks for a bucket.
 *
 *----------------------------------------------------------------------
 */

char *
TclpAlloc(unsigned int reqsize)
{
    Cache         *cachePtr = TclpGetAllocCache();
    Block         *blockPtr;
    register int   bucket;
    size_t  	   size;

    if (cachePtr == NULL) {
	cachePtr = GetCache();
    }
    
    /*
     * Increment the requested size to include room for 
     * the Block structure.  Call malloc() directly if the
     * required amount is greater than the largest block,
     * otherwise pop the smallest block large enough,
     * allocating more blocks if necessary.
     */

    blockPtr = NULL;     
    size = reqsize + sizeof(Block);
#if RCHECK
    ++size;
#endif
    if (size > MAXALLOC) {
	bucket = NBUCKETS;
    	blockPtr = malloc(size);
	if (blockPtr != NULL) {
	    cachePtr->nsysalloc += reqsize;
	}
    } else {
    	bucket = 0;
    	while (binfo[bucket].blocksize < size) {
    	    ++bucket;
    	}
    	if (cachePtr->buckets[bucket].nfree || GetBlocks(cachePtr, bucket)) {
	    blockPtr = cachePtr->buckets[bucket].firstPtr;
	    cachePtr->buckets[bucket].firstPtr = blockPtr->b_next;
	    --cachePtr->buckets[bucket].nfree;
    	    ++cachePtr->buckets[bucket].nget;
	    cachePtr->buckets[bucket].nrequest += reqsize;
	}
    }
    if (blockPtr == NULL) {
    	return NULL;
    }
    return Block2Ptr(blockPtr, bucket, reqsize);
}


/*
 *----------------------------------------------------------------------
 *
 *  TclpFree --
 *
 *	Return blocks to the thread block cache.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	May move blocks to shared cache.
 *
 *----------------------------------------------------------------------
 */

void
TclpFree(char *ptr)
{
    if (ptr != NULL) {
	Cache  *cachePtr = TclpGetAllocCache();
	Block *blockPtr;
	int bucket;

	if (cachePtr == NULL) {
	    cachePtr = GetCache();
	}
 
	/*
	 * Get the block back from the user pointer and
	 * call system free directly for large blocks.
	 * Otherwise, push the block back on the bucket and
	 * move blocks to the shared cache if there are now
	 * too many free.
	 */

	blockPtr = Ptr2Block(ptr);
	bucket = blockPtr->b_bucket;
	if (bucket == NBUCKETS) {
	    cachePtr->nsysalloc -= blockPtr->b_reqsize;
	    free(blockPtr);
	} else {
	    cachePtr->buckets[bucket].nrequest -= blockPtr->b_reqsize;
	    blockPtr->b_next = cachePtr->buckets[bucket].firstPtr;
	    cachePtr->buckets[bucket].firstPtr = blockPtr;
	    ++cachePtr->buckets[bucket].nfree;
	    ++cachePtr->buckets[bucket].nput;
	    if (cachePtr != sharedPtr &&
		    cachePtr->buckets[bucket].nfree > binfo[bucket].maxblocks) {
		PutBlocks(cachePtr, bucket, binfo[bucket].nmove);
	    }
	}
    }
}


/*
 *----------------------------------------------------------------------
 *
 *  TclpRealloc --
 *
 *	Re-allocate memory to a larger or smaller size.
 *
 * Results:
 *	Pointer to memory just beyond Block pointer.
 *
 * Side effects:
 *	Previous memory, if any, may be freed.
 *
 *----------------------------------------------------------------------
 */

char *
TclpRealloc(char *ptr, unsigned int reqsize)
{
    Cache *cachePtr = TclpGetAllocCache();
    Block *blockPtr;
    void *new;
    size_t size, min;
    int bucket;

    if (ptr == NULL) {
	return TclpAlloc(reqsize);
    }

    if (cachePtr == NULL) {
	cachePtr = GetCache();
    }

    /*
     * If the block is not a system block and fits in place,
     * simply return the existing pointer.  Otherwise, if the block
     * is a system block and the new size would also require a system
     * block, call realloc() directly.
     */

    blockPtr = Ptr2Block(ptr);
    size = reqsize + sizeof(Block);
#if RCHECK
    ++size;
#endif
    bucket = blockPtr->b_bucket;
    if (bucket != NBUCKETS) {
	if (bucket > 0) {
	    min = binfo[bucket-1].blocksize;
	} else {
	    min = 0;
	}
	if (size > min && size <= binfo[bucket].blocksize) {
	    cachePtr->buckets[bucket].nrequest -= blockPtr->b_reqsize;
	    cachePtr->buckets[bucket].nrequest += reqsize;
	    return Block2Ptr(blockPtr, bucket, reqsize);
	}
    } else if (size > MAXALLOC) {
	cachePtr->nsysalloc -= blockPtr->b_reqsize;
	cachePtr->nsysalloc += reqsize;
	blockPtr = realloc(blockPtr, size);
	if (blockPtr == NULL) {
	    return NULL;
	}
	return Block2Ptr(blockPtr, NBUCKETS, reqsize);
    }

    /*
     * Finally, perform an expensive malloc/copy/free.
     */

    new = TclpAlloc(reqsize);
    if (new != NULL) {
	if (reqsize > blockPtr->b_reqsize) {
	    reqsize = blockPtr->b_reqsize;
	}
    	memcpy(new, ptr, reqsize);
    	TclpFree(ptr);
    }
    return new;
}


/*
 *----------------------------------------------------------------------
 *
 * TclThreadAllocObj --
 *
 *	Allocate a Tcl_Obj from the per-thread cache.
 *
 * Results:
 *	Pointer to uninitialized Tcl_Obj.
 *
 * Side effects:
 *	May move Tcl_Obj's from shared cached or allocate new Tcl_Obj's
 *  	if list is empty.
 *
 *----------------------------------------------------------------------
 */

Tcl_Obj *
TclThreadAllocObj(void)
{
    register Cache *cachePtr = TclpGetAllocCache();
    register int nmove;
    register Tcl_Obj *objPtr;
    Tcl_Obj *newObjsPtr;

    if (cachePtr == NULL) {
	cachePtr = GetCache();
    }

    /*
     * Get this thread's obj list structure and move
     * or allocate new objs if necessary.
     */
     
    if (cachePtr->nobjs == 0) {
    	Tcl_MutexLock(objLockPtr);
	nmove = sharedPtr->nobjs;
	if (nmove > 0) {
	    if (nmove > NOBJALLOC) {
		nmove = NOBJALLOC;
	    }
	    MoveObjs(sharedPtr, cachePtr, nmove);
	}
    	Tcl_MutexUnlock(objLockPtr);
	if (cachePtr->nobjs == 0) {
	    cachePtr->nobjs = nmove = NOBJALLOC;
	    newObjsPtr = malloc(sizeof(Tcl_Obj) * nmove);
	    if (newObjsPtr == NULL) {
		panic("alloc: could not allocate %d new objects", nmove);
	    }
	    while (--nmove >= 0) {
		objPtr = &newObjsPtr[nmove];
		objPtr->internalRep.otherValuePtr = cachePtr->firstObjPtr;
		cachePtr->firstObjPtr = objPtr;
	    }
	}
    }

    /*
     * Pop the first object.
     */

    objPtr = cachePtr->firstObjPtr;
    cachePtr->firstObjPtr = objPtr->internalRep.otherValuePtr;
    --cachePtr->nobjs;
    return objPtr;
}


/*
 *----------------------------------------------------------------------
 *
 * TclThreadFreeObj --
 *
 *	Return a free Tcl_Obj to the per-thread cache.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	May move free Tcl_Obj's to shared list upon hitting high
 *  	water mark.
 *
 *----------------------------------------------------------------------
 */

void
TclThreadFreeObj(Tcl_Obj *objPtr)
{
    Cache *cachePtr = TclpGetAllocCache();

    if (cachePtr == NULL) {
	cachePtr = GetCache();
    }

    /*
     * Get this thread's list and push on the free Tcl_Obj.
     */
     
    objPtr->internalRep.otherValuePtr = cachePtr->firstObjPtr;
    cachePtr->firstObjPtr = objPtr;
    ++cachePtr->nobjs;
    
    /*
     * If the number of free objects has exceeded the high
     * water mark, move some blocks to the shared list.
     */
     
    if (cachePtr->nobjs > NOBJHIGH) {
	Tcl_MutexLock(objLockPtr);
	MoveObjs(cachePtr, sharedPtr, NOBJALLOC);
	Tcl_MutexUnlock(objLockPtr);
    }
}


/*
 *----------------------------------------------------------------------
 *
 * Tcl_GetMemoryInfo --
 *
 *	Return a list-of-lists of memory stats.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *  	List appended to given dstring.
 *
 *----------------------------------------------------------------------
 */

void
Tcl_GetMemoryInfo(Tcl_DString *dsPtr)
{
    Cache *cachePtr;
    char buf[200];
    int n;

    Tcl_MutexLock(listLockPtr);
    cachePtr = firstCachePtr;
    while (cachePtr != NULL) {
	Tcl_DStringStartSublist(dsPtr);
	if (cachePtr == sharedPtr) {
    	    Tcl_DStringAppendElement(dsPtr, "shared");
	} else {
	    sprintf(buf, "thread%d", (int) cachePtr->owner);
    	    Tcl_DStringAppendElement(dsPtr, buf);
	}
	for (n = 0; n < NBUCKETS; ++n) {
    	    sprintf(buf, "%d %d %d %d %d %d %d",
		(int) binfo[n].blocksize,
		cachePtr->buckets[n].nfree,
		cachePtr->buckets[n].nget,
		cachePtr->buckets[n].nput,
		cachePtr->buckets[n].nrequest,
		cachePtr->buckets[n].nlock,
		cachePtr->buckets[n].nwait);
	    Tcl_DStringAppendElement(dsPtr, buf);
	}
	Tcl_DStringEndSublist(dsPtr);
	    cachePtr = cachePtr->nextPtr;
    }
    Tcl_MutexUnlock(listLockPtr);
}


/*
 *----------------------------------------------------------------------
 *
 * MoveObjs --
 *
 *	Move Tcl_Obj's between caches.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *  	None.
 *
 *----------------------------------------------------------------------
 */

static void
MoveObjs(Cache *fromPtr, Cache *toPtr, int nmove)
{
    register Tcl_Obj *objPtr = fromPtr->firstObjPtr;
    Tcl_Obj *fromFirstObjPtr = objPtr;

    toPtr->nobjs += nmove;
    fromPtr->nobjs -= nmove;

    /*
     * Find the last object to be moved; set the next one
     * (the first one not to be moved) as the first object
     * in the 'from' cache.
     */

    while (--nmove) {
	objPtr = objPtr->internalRep.otherValuePtr;
    }
    fromPtr->firstObjPtr = objPtr->internalRep.otherValuePtr;    

    /*
     * Move all objects as a block - they are already linked to
     * each other, we just have to update the first and last.
     */

    objPtr->internalRep.otherValuePtr = toPtr->firstObjPtr;
    toPtr->firstObjPtr = fromFirstObjPtr;
}


/*
 *----------------------------------------------------------------------
 *
 *  Block2Ptr, Ptr2Block --
 *
 *	Convert between internal blocks and user pointers.
 *
 * Results:
 *	User pointer or internal block.
 *
 * Side effects:
 *	Invalid blocks will abort the server.
 *
 *----------------------------------------------------------------------
 */

static char *
Block2Ptr(Block *blockPtr, int bucket, unsigned int reqsize) 
{
    register void *ptr;

    blockPtr->b_magic1 = blockPtr->b_magic2 = MAGIC;
    blockPtr->b_bucket = bucket;
    blockPtr->b_reqsize = reqsize;
    ptr = ((void *) (blockPtr + 1));
#if RCHECK
    ((unsigned char *)(ptr))[reqsize] = MAGIC;
#endif
    return (char *) ptr;
}

static Block *
Ptr2Block(char *ptr)
{
    register Block *blockPtr;

    blockPtr = (((Block *) ptr) - 1);
    if (blockPtr->b_magic1 != MAGIC
#if RCHECK
	|| ((unsigned char *) ptr)[blockPtr->b_reqsize] != MAGIC
#endif
	|| blockPtr->b_magic2 != MAGIC) {
	panic("alloc: invalid block: %p: %x %x %x\n",
	    blockPtr, blockPtr->b_magic1, blockPtr->b_magic2,
	    ((unsigned char *) ptr)[blockPtr->b_reqsize]);
    }
    return blockPtr;
}


/*
 *----------------------------------------------------------------------
 *
 *  LockBucket, UnlockBucket --
 *
 *	Set/unset the lock to access a bucket in the shared cache.
 *
 * Results:
 *  	None.
 *
 * Side effects:
 *	Lock activity and contention are monitored globally and on
 *  	a per-cache basis.
 *
 *----------------------------------------------------------------------
 */

static void
LockBucket(Cache *cachePtr, int bucket)
{
#if 0
    if (Tcl_MutexTryLock(binfo[bucket].lockPtr) != TCL_OK) {
	Tcl_MutexLock(binfo[bucket].lockPtr);
    	++cachePtr->buckets[bucket].nwait;
    	++sharedPtr->buckets[bucket].nwait;
    }
#else
    Tcl_MutexLock(binfo[bucket].lockPtr);
#endif
    ++cachePtr->buckets[bucket].nlock;
    ++sharedPtr->buckets[bucket].nlock;
}


static void
UnlockBucket(Cache *cachePtr, int bucket)
{
    Tcl_MutexUnlock(binfo[bucket].lockPtr);
}


/*
 *----------------------------------------------------------------------
 *
 *  PutBlocks --
 *
 *	Return unused blocks to the shared cache.
 *
 * Results:
 *	None.
 *
 * Side effects:
 *	None.
 *
 *----------------------------------------------------------------------
 */

static void
PutBlocks(Cache *cachePtr, int bucket, int nmove)
{
    register Block *lastPtr, *firstPtr;
    register int n = nmove;

    /*
     * Before acquiring the lock, walk the block list to find
     * the last block to be moved.
     */

    firstPtr = lastPtr = cachePtr->buckets[bucket].firstPtr;
    while (--n > 0) {
	lastPtr = lastPtr->b_next;
    }
    cachePtr->buckets[bucket].firstPtr = lastPtr->b_next;
    cachePtr->buckets[bucket].nfree -= nmove;

    /*
     * Aquire the lock and place the list of blocks at the front
     * of the shared cache bucket.
     */

    LockBucket(cachePtr, bucket);
    lastPtr->b_next = sharedPtr->buckets[bucket].firstPtr;
    sharedPtr->buckets[bucket].firstPtr = firstPtr;
    sharedPtr->buckets[bucket].nfree += nmove;
    UnlockBucket(cachePtr, bucket);
}


/*
 *----------------------------------------------------------------------
 *
 *  GetBlocks --
 *
 *	Get more blocks for a bucket.
 *
 * Results:
 *	1 if blocks where allocated, 0 otherwise.
 *
 * Side effects:
 *	Cache may be filled with available blocks.
 *
 *----------------------------------------------------------------------
 */

static int
GetBlocks(Cache *cachePtr, int bucket)
{
    register Block *blockPtr;
    register int n;
    register size_t size;

    /*
     * First, atttempt to move blocks from the shared cache.  Note
     * the potentially dirty read of nfree before acquiring the lock
     * which is a slight performance enhancement.  The value is
     * verified after the lock is actually acquired.
     */
     
    if (cachePtr != sharedPtr && sharedPtr->buckets[bucket].nfree > 0) {
	LockBucket(cachePtr, bucket);
	if (sharedPtr->buckets[bucket].nfree > 0) {

	    /*
	     * Either move the entire list or walk the list to find
	     * the last block to move.
	     */

	    n = binfo[bucket].nmove;
	    if (n >= sharedPtr->buckets[bucket].nfree) {
		cachePtr->buckets[bucket].firstPtr =
		    sharedPtr->buckets[bucket].firstPtr;
		cachePtr->buckets[bucket].nfree =
		    sharedPtr->buckets[bucket].nfree;
		sharedPtr->buckets[bucket].firstPtr = NULL;
		sharedPtr->buckets[bucket].nfree = 0;
	    } else {
		blockPtr = sharedPtr->buckets[bucket].firstPtr;
		cachePtr->buckets[bucket].firstPtr = blockPtr;
		sharedPtr->buckets[bucket].nfree -= n;
		cachePtr->buckets[bucket].nfree = n;
		while (--n > 0) {
    		    blockPtr = blockPtr->b_next;
		}
		sharedPtr->buckets[bucket].firstPtr = blockPtr->b_next;
		blockPtr->b_next = NULL;
	    }
	}
	UnlockBucket(cachePtr, bucket);
    }
    
    if (cachePtr->buckets[bucket].nfree == 0) {

	/*
	 * If no blocks could be moved from shared, first look for a
	 * larger block in this cache to split up.
	 */

    	blockPtr = NULL;
	n = NBUCKETS;
	size = 0; /* lint */
	while (--n > bucket) {
    	    if (cachePtr->buckets[n].nfree > 0) {
		size = binfo[n].blocksize;
		blockPtr = cachePtr->buckets[n].firstPtr;
		cachePtr->buckets[n].firstPtr = blockPtr->b_next;
		--cachePtr->buckets[n].nfree;
		break;
	    }
	}

	/*
	 * Otherwise, allocate a big new block directly.
	 */

	if (blockPtr == NULL) {
	    size = MAXALLOC;
	    blockPtr = malloc(size);
	    if (blockPtr == NULL) {
		return 0;
	    }
	}

	/*
	 * Split the larger block into smaller blocks for this bucket.
	 */

	n = size / binfo[bucket].blocksize;
	cachePtr->buckets[bucket].nfree = n;
	cachePtr->buckets[bucket].firstPtr = blockPtr;
	while (--n > 0) {
	    blockPtr->b_next = (Block *) 
		((char *) blockPtr + binfo[bucket].blocksize);
	    blockPtr = blockPtr->b_next;
	}
	blockPtr->b_next = NULL;
    }
    return 1;
}

#endif /* TCL_THREADS */
@


1.3.48.1
log
@gdb/insight for ARCompact (from Richard Stuckey)
@
text
@d14 1
a14 1
 * RCS: @@(#) $Id: tclThreadAlloc.c,v 1.3 2003/01/21 19:40:06 hunt Exp $ */
@


1.2
log
@touched all sources to ease next import
@
text
@d14 1
a14 1
 * RCS: @@(#) $Id: tclThreadAlloc.c,v 1.1 2002/09/24 19:56:08 kseitz Exp $ */
@


1.1
log
@Initial revision
@
text
@d14 1
a14 1
 * RCS: @@(#) $Id: tclThreadAlloc.c,v 1.4 2002/08/26 13:05:56 msofer Exp $ */
@


1.1.6.1
log
@Merge drow-cplus-branch to:
  cvs rtag -D 2003-12-14 00:00:00 UTC drow-cplus-merge-20031214 gdb+dejagnu
@
text
@d14 1
a14 1
 * RCS: @@(#) $Id: tclThreadAlloc.c,v 1.3 2003/01/21 19:40:06 hunt Exp $ */
@


1.1.4.1
log
@Merge with kseitz_interps-20020930-merge.
@
text
@d14 1
a14 1
 * RCS: @@(#) $Id: tclThreadAlloc.c,v 1.1 2002/09/24 19:56:08 kseitz Exp $ */
@


1.1.2.1
log
@Merge with mainline; tag is carlton_dictionary-20020927-merge
@
text
@d14 1
a14 1
 * RCS: @@(#) $Id: tclThreadAlloc.c,v 1.1.1.1 2002/09/24 19:56:08 kseitz Exp $ */
@


1.1.2.2
log
@Merge with mainline; merge tag carlton_dictionary-20021011-merge.
@
text
@d14 1
a14 1
 * RCS: @@(#) $Id: tclThreadAlloc.c,v 1.1 2002/09/24 19:56:08 kseitz Exp $ */
@


1.1.2.3
log
@2002-12-23  David Carlton  <carlton@@math.stanford.edu>

	* Merge from mainline; tag is carlton_dictionary-20021223-merge.
@
text
@d14 1
a14 1
 * RCS: @@(#) $Id: tclThreadAlloc.c,v 1.1.2.2 2002/10/11 22:23:10 carlton Exp $ */
@


1.1.2.4
log
@2003-02-07  David Carlton  <carlton@@math.stanford.edu>

	* Merge with mainline; tag is carlton_dictionary-20030207-merge.
@
text
@d14 1
a14 1
 * RCS: @@(#) $Id: tclThreadAlloc.c,v 1.1.2.3 2002/12/23 19:40:25 carlton Exp $ */
@


1.1.1.1
log
@import tcl 8.4.0
@
text
@@


1.1.1.1.2.1
log
@Import Tcl 8.4.0 into mainline-like sources. Simplest Tcl build possible for
cygwin.
@
text
@d20 1
a20 1
#if defined(WIN32) || defined(__CYGWIN__)
d279 1
a279 1
#if defined(WIN32) || defined(__CYGWIN__)
@

